python imagenet-distributed.py  -n 2 -g 1 -nr 1 --epochs 2
=> creating model 'resnet18'
Epoch: [0][  99/2503]	Loss 6.6163e+00 (6.8352e+00)	Acc@1   0.78 (  0.50)	Acc@5   2.34 (  1.95)
Epoch: [0][ 199/2503]	Loss 6.3438e+00 (6.6356e+00)	Acc@1   1.17 (  0.79)	Acc@5   6.25 (  3.06)
Epoch: [0][ 299/2503]	Loss 6.0926e+00 (6.4794e+00)	Acc@1   2.34 (  1.14)	Acc@5   7.42 (  4.20)
Epoch: [0][ 399/2503]	Loss 5.8460e+00 (6.3574e+00)	Acc@1   1.95 (  1.49)	Acc@5   8.59 (  5.23)
Epoch: [0][ 499/2503]	Loss 5.7688e+00 (6.2440e+00)	Acc@1   4.69 (  1.87)	Acc@5  11.33 (  6.44)
Epoch: [0][ 599/2503]	Loss 5.5797e+00 (6.1455e+00)	Acc@1   3.52 (  2.26)	Acc@5  13.67 (  7.53)
Epoch: [0][ 699/2503]	Loss 5.4565e+00 (6.0479e+00)	Acc@1   5.47 (  2.73)	Acc@5  15.23 (  8.71)
Epoch: [0][ 799/2503]	Loss 5.2538e+00 (5.9618e+00)	Acc@1   7.03 (  3.13)	Acc@5  19.14 (  9.82)
Epoch: [0][ 899/2503]	Loss 5.2312e+00 (5.8800e+00)	Acc@1   7.03 (  3.59)	Acc@5  17.97 ( 10.93)
Epoch: [0][ 999/2503]	Loss 5.0965e+00 (5.8018e+00)	Acc@1   7.42 (  4.05)	Acc@5  23.05 ( 12.05)
Epoch: [0][1099/2503]	Loss 4.5770e+00 (5.7260e+00)	Acc@1  14.45 (  4.54)	Acc@5  31.64 ( 13.18)
Epoch: [0][1199/2503]	Loss 4.8192e+00 (5.6541e+00)	Acc@1  12.11 (  5.05)	Acc@5  28.12 ( 14.31)
Epoch: [0][1299/2503]	Loss 4.7115e+00 (5.5836e+00)	Acc@1  12.11 (  5.57)	Acc@5  28.91 ( 15.44)
Epoch: [0][1399/2503]	Loss 4.7900e+00 (5.5163e+00)	Acc@1   8.98 (  6.09)	Acc@5  28.12 ( 16.52)
Epoch: [0][1499/2503]	Loss 4.5555e+00 (5.4500e+00)	Acc@1  14.45 (  6.61)	Acc@5  30.47 ( 17.61)
Epoch: [0][1599/2503]	Loss 4.6824e+00 (5.3856e+00)	Acc@1  15.23 (  7.18)	Acc@5  32.42 ( 18.69)
Epoch: [0][1699/2503]	Loss 4.2751e+00 (5.3260e+00)	Acc@1  16.80 (  7.70)	Acc@5  35.94 ( 19.70)
Epoch: [0][1799/2503]	Loss 4.1562e+00 (5.2673e+00)	Acc@1  19.92 (  8.23)	Acc@5  38.67 ( 20.69)
Epoch: [0][1899/2503]	Loss 4.1255e+00 (5.2113e+00)	Acc@1  20.70 (  8.75)	Acc@5  40.62 ( 21.66)
Epoch: [0][1999/2503]	Loss 3.9700e+00 (5.1580e+00)	Acc@1  23.05 (  9.27)	Acc@5  40.23 ( 22.59)
Epoch: [0][2099/2503]	Loss 4.0464e+00 (5.1057e+00)	Acc@1  21.88 (  9.79)	Acc@5  41.80 ( 23.52)
Epoch: [0][2199/2503]	Loss 4.1736e+00 (5.0551e+00)	Acc@1  20.70 ( 10.28)	Acc@5  40.23 ( 24.41)
Epoch: [0][2299/2503]	Loss 3.9342e+00 (5.0064e+00)	Acc@1  19.92 ( 10.78)	Acc@5  42.58 ( 25.27)
Epoch: [0][2399/2503]	Loss 3.9659e+00 (4.9590e+00)	Acc@1  18.75 ( 11.29)	Acc@5  43.36 ( 26.10)
Epoch: [0][2499/2503]	Loss 3.5143e+00 (4.9130e+00)	Acc@1  30.08 ( 11.78)	Acc@5  51.56 ( 26.93)
 * Train Acc@1 11.789 Train Acc@5 26.954
Epoch: [0][ 99/391]	Loss 4.4231e+00 (4.0473e+00)	Acc@1  17.19 ( 20.75)	Acc@5  35.94 ( 43.09)
Epoch: [0][199/391]	Loss 3.5522e+00 (4.0532e+00)	Acc@1  25.00 ( 21.16)	Acc@5  48.44 ( 42.80)
Epoch: [0][299/391]	Loss 4.1424e+00 (4.0540e+00)	Acc@1  14.06 ( 20.70)	Acc@5  40.62 ( 42.77)
 * Val Acc@1 20.696 Val Acc@5 43.088
Epoch: [1][  99/2503]	Loss 3.6149e+00 (3.7477e+00)	Acc@1  27.73 ( 24.61)	Acc@5  49.22 ( 47.70)
Epoch: [1][ 199/2503]	Loss 3.8465e+00 (3.7240e+00)	Acc@1  28.12 ( 25.00)	Acc@5  45.70 ( 48.20)
Epoch: [1][ 299/2503]	Loss 3.7526e+00 (3.6934e+00)	Acc@1  25.39 ( 25.46)	Acc@5  46.09 ( 48.74)
Epoch: [1][ 399/2503]	Loss 3.4325e+00 (3.6714e+00)	Acc@1  29.30 ( 25.74)	Acc@5  51.56 ( 49.10)
Epoch: [1][ 499/2503]	Loss 3.4902e+00 (3.6525e+00)	Acc@1  27.34 ( 26.08)	Acc@5  53.12 ( 49.48)
Epoch: [1][ 599/2503]	Loss 3.3971e+00 (3.6285e+00)	Acc@1  28.91 ( 26.43)	Acc@5  51.56 ( 49.96)
Epoch: [1][ 699/2503]	Loss 3.4954e+00 (3.6078e+00)	Acc@1  28.52 ( 26.70)	Acc@5  53.12 ( 50.34)
Epoch: [1][ 799/2503]	Loss 3.5430e+00 (3.5859e+00)	Acc@1  30.47 ( 27.05)	Acc@5  52.73 ( 50.74)
Epoch: [1][ 899/2503]	Loss 3.3403e+00 (3.5660e+00)	Acc@1  25.78 ( 27.31)	Acc@5  51.95 ( 51.10)
Epoch: [1][ 999/2503]	Loss 3.4677e+00 (3.5477e+00)	Acc@1  30.08 ( 27.58)	Acc@5  49.61 ( 51.37)
Epoch: [1][1099/2503]	Loss 3.2505e+00 (3.5267e+00)	Acc@1  32.03 ( 27.91)	Acc@5  55.47 ( 51.75)
Epoch: [1][1199/2503]	Loss 3.2256e+00 (3.5056e+00)	Acc@1  30.86 ( 28.21)	Acc@5  54.30 ( 52.17)
Epoch: [1][1299/2503]	Loss 3.4284e+00 (3.4825e+00)	Acc@1  29.69 ( 28.57)	Acc@5  54.30 ( 52.58)
Epoch: [1][1399/2503]	Loss 2.9986e+00 (3.4595e+00)	Acc@1  32.42 ( 28.94)	Acc@5  58.59 ( 53.01)
Epoch: [1][1499/2503]	Loss 3.1779e+00 (3.4356e+00)	Acc@1  34.77 ( 29.33)	Acc@5  56.25 ( 53.43)
Epoch: [1][1599/2503]	Loss 3.0705e+00 (3.4132e+00)	Acc@1  33.98 ( 29.69)	Acc@5  62.89 ( 53.84)
Epoch: [1][1699/2503]	Loss 2.9511e+00 (3.3892e+00)	Acc@1  31.25 ( 30.09)	Acc@5  63.67 ( 54.25)
Epoch: [1][1799/2503]	Loss 2.8041e+00 (3.3657e+00)	Acc@1  35.94 ( 30.49)	Acc@5  63.67 ( 54.67)
Epoch: [1][1899/2503]	Loss 2.7770e+00 (3.3414e+00)	Acc@1  39.84 ( 30.92)	Acc@5  66.02 ( 55.09)
Epoch: [1][1999/2503]	Loss 2.9246e+00 (3.3178e+00)	Acc@1  38.28 ( 31.31)	Acc@5  62.50 ( 55.52)
Epoch: [1][2099/2503]	Loss 2.7831e+00 (3.2939e+00)	Acc@1  42.19 ( 31.71)	Acc@5  64.84 ( 55.95)
Epoch: [1][2199/2503]	Loss 2.6440e+00 (3.2714e+00)	Acc@1  39.45 ( 32.10)	Acc@5  67.97 ( 56.34)
Epoch: [1][2299/2503]	Loss 2.7794e+00 (3.2488e+00)	Acc@1  40.23 ( 32.49)	Acc@5  62.50 ( 56.73)
Epoch: [1][2399/2503]	Loss 2.6064e+00 (3.2267e+00)	Acc@1  45.31 ( 32.88)	Acc@5  69.92 ( 57.13)
Epoch: [1][2499/2503]	Loss 2.8808e+00 (3.2069e+00)	Acc@1  43.36 ( 33.21)	Acc@5  64.84 ( 57.46)
 * Train Acc@1 33.217 Train Acc@5 57.472
Epoch: [1][ 99/391]	Loss 2.4598e+00 (2.3767e+00)	Acc@1  48.44 ( 46.84)	Acc@5  67.19 ( 72.11)
Epoch: [1][199/391]	Loss 1.8446e+00 (2.3750e+00)	Acc@1  54.69 ( 46.84)	Acc@5  78.12 ( 72.02)
Epoch: [1][299/391]	Loss 2.3966e+00 (2.3767e+00)	Acc@1  43.75 ( 46.46)	Acc@5  71.88 ( 71.93)
 * Val Acc@1 46.780 Val Acc@5 72.148
Training complete in: 0:48:45.460161
(pytorch_latest_p37) ubuntu@ip-172-31-6-102:~/work/v3/week09/hw$ 
