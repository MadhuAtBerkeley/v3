(pytorch_p37) ubuntu@ip-172-31-2-157:~/work/v3/week09/hw$ python imagenet-distributed.py  -n 2 -g 1 -nr 0 --epochs 6
=> creating model 'resnet18'
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [0][  99/2503]	Loss 6.7836e+00 (6.8626e+00)	Acc@1   0.39 (  0.38)	Acc@5   1.56 (  1.55)
Epoch: [0][ 199/2503]	Loss 6.2041e+00 (6.6443e+00)	Acc@1   3.91 (  0.86)	Acc@5   8.59 (  2.98)
Epoch: [0][ 299/2503]	Loss 6.0443e+00 (6.4763e+00)	Acc@1   2.73 (  1.26)	Acc@5   6.25 (  4.26)
Epoch: [0][ 399/2503]	Loss 5.9162e+00 (6.3393e+00)	Acc@1   2.34 (  1.67)	Acc@5  10.16 (  5.60)
Epoch: [0][ 499/2503]	Loss 5.6853e+00 (6.2253e+00)	Acc@1   3.91 (  2.04)	Acc@5  12.11 (  6.73)
Epoch: [0][ 599/2503]	Loss 5.5342e+00 (6.1235e+00)	Acc@1   5.47 (  2.43)	Acc@5  13.67 (  7.82)
Epoch: [0][ 699/2503]	Loss 5.3861e+00 (6.0355e+00)	Acc@1   5.08 (  2.82)	Acc@5  13.28 (  8.91)
Epoch: [0][ 799/2503]	Loss 5.2828e+00 (5.9562e+00)	Acc@1   5.47 (  3.18)	Acc@5  19.14 (  9.93)
Epoch: [0][ 899/2503]	Loss 5.0794e+00 (5.8828e+00)	Acc@1   7.03 (  3.57)	Acc@5  25.00 ( 10.91)
Epoch: [0][ 999/2503]	Loss 5.0041e+00 (5.8102e+00)	Acc@1  10.94 (  3.98)	Acc@5  23.83 ( 11.90)
Epoch: [0][1099/2503]	Loss 5.2549e+00 (5.7436e+00)	Acc@1  10.16 (  4.40)	Acc@5  21.09 ( 12.88)
Epoch: [0][1199/2503]	Loss 4.8480e+00 (5.6763e+00)	Acc@1  12.89 (  4.85)	Acc@5  28.52 ( 13.90)
Epoch: [0][1299/2503]	Loss 4.8318e+00 (5.6117e+00)	Acc@1   8.59 (  5.29)	Acc@5  25.78 ( 14.92)
Epoch: [0][1399/2503]	Loss 4.7800e+00 (5.5500e+00)	Acc@1  10.16 (  5.75)	Acc@5  25.78 ( 15.88)
Epoch: [0][1499/2503]	Loss 4.6388e+00 (5.4917e+00)	Acc@1  12.50 (  6.19)	Acc@5  28.12 ( 16.81)
Epoch: [0][1599/2503]	Loss 4.4799e+00 (5.4340e+00)	Acc@1  12.50 (  6.67)	Acc@5  32.81 ( 17.76)
Epoch: [0][1699/2503]	Loss 4.5550e+00 (5.3796e+00)	Acc@1  14.84 (  7.11)	Acc@5  33.59 ( 18.66)
Epoch: [0][1799/2503]	Loss 4.4016e+00 (5.3273e+00)	Acc@1  14.45 (  7.56)	Acc@5  33.20 ( 19.54)
Epoch: [0][1899/2503]	Loss 4.1797e+00 (5.2757e+00)	Acc@1  17.58 (  8.02)	Acc@5  33.59 ( 20.42)
Epoch: [0][1999/2503]	Loss 4.2750e+00 (5.2266e+00)	Acc@1  15.62 (  8.47)	Acc@5  37.89 ( 21.26)
Epoch: [0][2099/2503]	Loss 4.2451e+00 (5.1790e+00)	Acc@1  16.80 (  8.90)	Acc@5  35.16 ( 22.09)
Epoch: [0][2199/2503]	Loss 4.3715e+00 (5.1329e+00)	Acc@1  15.62 (  9.34)	Acc@5  33.98 ( 22.90)
Epoch: [0][2299/2503]	Loss 3.9574e+00 (5.0866e+00)	Acc@1  19.92 (  9.81)	Acc@5  41.80 ( 23.70)
Epoch: [0][2399/2503]	Loss 4.1160e+00 (5.0430e+00)	Acc@1  17.97 ( 10.24)	Acc@5  39.84 ( 24.48)
Epoch: [0][2499/2503]	Loss 3.9453e+00 (4.9996e+00)	Acc@1  24.22 ( 10.67)	Acc@5  44.53 ( 25.25)
 * Train Acc@1 10.680 Train Acc@5 25.264
Epoch: [0][ 99/391]	Loss 4.6301e+00 (4.0842e+00)	Acc@1  17.19 ( 19.53)	Acc@5  29.69 ( 40.95)
Epoch: [0][199/391]	Loss 3.8515e+00 (4.0831e+00)	Acc@1  20.31 ( 19.81)	Acc@5  46.88 ( 41.30)
Epoch: [0][299/391]	Loss 4.0268e+00 (4.0844e+00)	Acc@1  18.75 ( 20.03)	Acc@5  46.88 ( 41.52)
 * Val Acc@1 19.864 Val Acc@5 41.400
Epoch: [1][  99/2503]	Loss 3.7333e+00 (3.8875e+00)	Acc@1  22.27 ( 22.12)	Acc@5  46.48 ( 44.92)
Epoch: [1][ 199/2503]	Loss 3.8130e+00 (3.8813e+00)	Acc@1  21.88 ( 22.39)	Acc@5  48.05 ( 45.24)
Epoch: [1][ 299/2503]	Loss 3.7522e+00 (3.8655e+00)	Acc@1  25.00 ( 22.60)	Acc@5  46.09 ( 45.57)
Epoch: [1][ 399/2503]	Loss 3.7597e+00 (3.8429e+00)	Acc@1  22.66 ( 22.94)	Acc@5  46.88 ( 45.93)
Epoch: [1][ 499/2503]	Loss 3.2522e+00 (3.8207e+00)	Acc@1  32.81 ( 23.28)	Acc@5  55.86 ( 46.34)
Epoch: [1][ 599/2503]	Loss 3.7257e+00 (3.8006e+00)	Acc@1  24.22 ( 23.54)	Acc@5  50.78 ( 46.75)
Epoch: [1][ 699/2503]	Loss 3.6277e+00 (3.7818e+00)	Acc@1  26.17 ( 23.84)	Acc@5  48.05 ( 47.11)
Epoch: [1][ 799/2503]	Loss 3.4377e+00 (3.7645e+00)	Acc@1  29.69 ( 24.11)	Acc@5  51.17 ( 47.41)
Epoch: [1][ 899/2503]	Loss 3.4578e+00 (3.7431e+00)	Acc@1  25.39 ( 24.47)	Acc@5  53.52 ( 47.82)
Epoch: [1][ 999/2503]	Loss 3.4168e+00 (3.7257e+00)	Acc@1  28.12 ( 24.74)	Acc@5  51.56 ( 48.13)
Epoch: [1][1099/2503]	Loss 3.4531e+00 (3.7093e+00)	Acc@1  30.08 ( 24.97)	Acc@5  52.34 ( 48.43)
Epoch: [1][1199/2503]	Loss 3.3654e+00 (3.6946e+00)	Acc@1  30.47 ( 25.18)	Acc@5  55.08 ( 48.72)
Epoch: [1][1299/2503]	Loss 3.3905e+00 (3.6791e+00)	Acc@1  26.56 ( 25.39)	Acc@5  55.47 ( 49.03)
Epoch: [1][1399/2503]	Loss 3.2594e+00 (3.6639e+00)	Acc@1  31.25 ( 25.61)	Acc@5  53.12 ( 49.31)
Epoch: [1][1499/2503]	Loss 3.3767e+00 (3.6480e+00)	Acc@1  30.47 ( 25.85)	Acc@5  54.69 ( 49.61)
Epoch: [1][1599/2503]	Loss 3.4172e+00 (3.6330e+00)	Acc@1  27.73 ( 26.08)	Acc@5  52.34 ( 49.88)
Epoch: [1][1699/2503]	Loss 3.4541e+00 (3.6182e+00)	Acc@1  28.91 ( 26.31)	Acc@5  50.78 ( 50.15)
Epoch: [1][1799/2503]	Loss 3.2153e+00 (3.6048e+00)	Acc@1  32.42 ( 26.53)	Acc@5  54.30 ( 50.40)
Epoch: [1][1899/2503]	Loss 3.3262e+00 (3.5911e+00)	Acc@1  31.25 ( 26.77)	Acc@5  53.91 ( 50.67)
Epoch: [1][1999/2503]	Loss 3.2781e+00 (3.5777e+00)	Acc@1  35.94 ( 26.99)	Acc@5  54.30 ( 50.92)
Epoch: [1][2099/2503]	Loss 3.3752e+00 (3.5644e+00)	Acc@1  31.25 ( 27.20)	Acc@5  53.52 ( 51.17)
Epoch: [1][2199/2503]	Loss 3.1966e+00 (3.5513e+00)	Acc@1  36.72 ( 27.44)	Acc@5  58.59 ( 51.43)
Epoch: [1][2299/2503]	Loss 3.3160e+00 (3.5391e+00)	Acc@1  29.30 ( 27.64)	Acc@5  52.73 ( 51.64)
Epoch: [1][2399/2503]	Loss 3.1036e+00 (3.5260e+00)	Acc@1  34.77 ( 27.84)	Acc@5  57.42 ( 51.88)
Epoch: [1][2499/2503]	Loss 3.4758e+00 (3.5147e+00)	Acc@1  27.34 ( 28.02)	Acc@5  50.39 ( 52.09)
 * Train Acc@1 28.026 Train Acc@5 52.101
Epoch: [1][ 99/391]	Loss 3.6689e+00 (3.2637e+00)	Acc@1  21.88 ( 31.17)	Acc@5  45.31 ( 56.94)
Epoch: [1][199/391]	Loss 3.2239e+00 (3.2742e+00)	Acc@1  28.12 ( 30.79)	Acc@5  59.38 ( 56.57)
Epoch: [1][299/391]	Loss 2.9691e+00 (3.2604e+00)	Acc@1  31.25 ( 31.19)	Acc@5  60.94 ( 56.85)
 * Val Acc@1 31.064 Val Acc@5 56.772
Epoch: [2][  99/2503]	Loss 3.0505e+00 (3.1504e+00)	Acc@1  35.55 ( 34.02)	Acc@5  59.77 ( 58.38)
Epoch: [2][ 199/2503]	Loss 3.2202e+00 (3.1520e+00)	Acc@1  30.86 ( 33.82)	Acc@5  60.16 ( 58.41)
Epoch: [2][ 299/2503]	Loss 2.9546e+00 (3.1492e+00)	Acc@1  33.59 ( 33.88)	Acc@5  60.55 ( 58.58)
Epoch: [2][ 399/2503]	Loss 2.8963e+00 (3.1436e+00)	Acc@1  38.28 ( 33.86)	Acc@5  64.06 ( 58.68)
Epoch: [2][ 499/2503]	Loss 3.3134e+00 (3.1342e+00)	Acc@1  26.95 ( 33.97)	Acc@5  54.69 ( 58.90)
Epoch: [2][ 599/2503]	Loss 3.1907e+00 (3.1259e+00)	Acc@1  32.81 ( 34.14)	Acc@5  56.64 ( 59.08)
Epoch: [2][ 699/2503]	Loss 3.1018e+00 (3.1150e+00)	Acc@1  37.11 ( 34.33)	Acc@5  60.16 ( 59.30)
Epoch: [2][ 799/2503]	Loss 3.0002e+00 (3.1072e+00)	Acc@1  38.28 ( 34.47)	Acc@5  57.42 ( 59.46)
Epoch: [2][ 899/2503]	Loss 2.9895e+00 (3.1038e+00)	Acc@1  35.55 ( 34.50)	Acc@5  62.89 ( 59.54)
Epoch: [2][ 999/2503]	Loss 3.1738e+00 (3.0995e+00)	Acc@1  33.59 ( 34.56)	Acc@5  57.42 ( 59.64)
Epoch: [2][1099/2503]	Loss 3.0401e+00 (3.0942e+00)	Acc@1  30.86 ( 34.65)	Acc@5  62.89 ( 59.75)
Epoch: [2][1199/2503]	Loss 3.2702e+00 (3.0907e+00)	Acc@1  30.47 ( 34.72)	Acc@5  59.38 ( 59.84)
Epoch: [2][1299/2503]	Loss 3.0476e+00 (3.0878e+00)	Acc@1  33.98 ( 34.76)	Acc@5  60.55 ( 59.89)
Epoch: [2][1399/2503]	Loss 2.9412e+00 (3.0830e+00)	Acc@1  37.89 ( 34.86)	Acc@5  64.06 ( 59.95)
Epoch: [2][1499/2503]	Loss 3.1030e+00 (3.0787e+00)	Acc@1  36.33 ( 34.96)	Acc@5  57.42 ( 60.03)
Epoch: [2][1599/2503]	Loss 2.7970e+00 (3.0755e+00)	Acc@1  36.72 ( 35.01)	Acc@5  64.45 ( 60.07)
Epoch: [2][1699/2503]	Loss 3.1479e+00 (3.0703e+00)	Acc@1  34.38 ( 35.10)	Acc@5  59.77 ( 60.16)
Epoch: [2][1799/2503]	Loss 2.9486e+00 (3.0659e+00)	Acc@1  35.94 ( 35.18)	Acc@5  60.55 ( 60.23)
Epoch: [2][1899/2503]	Loss 2.9623e+00 (3.0620e+00)	Acc@1  37.50 ( 35.25)	Acc@5  64.84 ( 60.29)
Epoch: [2][1999/2503]	Loss 2.6362e+00 (3.0551e+00)	Acc@1  45.70 ( 35.37)	Acc@5  68.75 ( 60.42)
Epoch: [2][2099/2503]	Loss 2.7424e+00 (3.0503e+00)	Acc@1  40.23 ( 35.45)	Acc@5  67.19 ( 60.49)
Epoch: [2][2199/2503]	Loss 3.0368e+00 (3.0438e+00)	Acc@1  37.50 ( 35.56)	Acc@5  61.33 ( 60.62)
Epoch: [2][2299/2503]	Loss 2.9242e+00 (3.0395e+00)	Acc@1  39.45 ( 35.62)	Acc@5  64.84 ( 60.70)
Epoch: [2][2399/2503]	Loss 2.8351e+00 (3.0344e+00)	Acc@1  40.62 ( 35.71)	Acc@5  61.33 ( 60.78)
Epoch: [2][2499/2503]	Loss 2.5467e+00 (3.0289e+00)	Acc@1  44.92 ( 35.81)	Acc@5  69.14 ( 60.88)
 * Train Acc@1 35.811 Train Acc@5 60.879
Epoch: [2][ 99/391]	Loss 2.6537e+00 (2.7802e+00)	Acc@1  40.62 ( 38.41)	Acc@5  64.06 ( 65.16)
Epoch: [2][199/391]	Loss 2.7669e+00 (2.8140e+00)	Acc@1  35.94 ( 37.81)	Acc@5  62.50 ( 64.77)
Epoch: [2][299/391]	Loss 2.8443e+00 (2.8136e+00)	Acc@1  42.19 ( 38.20)	Acc@5  60.94 ( 64.93)
 * Val Acc@1 38.124 Val Acc@5 64.916
Epoch: [3][  99/2503]	Loss 2.9033e+00 (2.8556e+00)	Acc@1  38.67 ( 38.79)	Acc@5  63.28 ( 64.00)
Epoch: [3][ 199/2503]	Loss 2.8441e+00 (2.8461e+00)	Acc@1  35.16 ( 38.83)	Acc@5  67.97 ( 64.09)
Epoch: [3][ 299/2503]	Loss 2.6802e+00 (2.8389e+00)	Acc@1  44.53 ( 39.09)	Acc@5  67.58 ( 64.21)
Epoch: [3][ 399/2503]	Loss 2.7093e+00 (2.8363e+00)	Acc@1  42.58 ( 39.12)	Acc@5  66.02 ( 64.25)
Epoch: [3][ 499/2503]	Loss 3.1899e+00 (2.8318e+00)	Acc@1  32.81 ( 39.15)	Acc@5  55.86 ( 64.31)
Epoch: [3][ 599/2503]	Loss 3.0694e+00 (2.8284e+00)	Acc@1  34.38 ( 39.25)	Acc@5  61.72 ( 64.37)
Epoch: [3][ 699/2503]	Loss 2.4319e+00 (2.8258e+00)	Acc@1  48.05 ( 39.31)	Acc@5  69.53 ( 64.39)
Epoch: [3][ 799/2503]	Loss 3.1262e+00 (2.8277e+00)	Acc@1  35.94 ( 39.23)	Acc@5  60.16 ( 64.37)
Epoch: [3][ 899/2503]	Loss 2.6579e+00 (2.8241e+00)	Acc@1  46.09 ( 39.33)	Acc@5  66.80 ( 64.45)
Epoch: [3][ 999/2503]	Loss 2.6342e+00 (2.8193e+00)	Acc@1  40.62 ( 39.43)	Acc@5  71.09 ( 64.52)
Epoch: [3][1099/2503]	Loss 2.7633e+00 (2.8162e+00)	Acc@1  39.45 ( 39.46)	Acc@5  67.58 ( 64.57)
Epoch: [3][1199/2503]	Loss 2.7129e+00 (2.8131e+00)	Acc@1  42.19 ( 39.53)	Acc@5  65.62 ( 64.63)
Epoch: [3][1299/2503]	Loss 2.6411e+00 (2.8101e+00)	Acc@1  45.70 ( 39.59)	Acc@5  66.80 ( 64.72)
Epoch: [3][1399/2503]	Loss 2.7467e+00 (2.8094e+00)	Acc@1  42.19 ( 39.62)	Acc@5  64.06 ( 64.72)
Epoch: [3][1499/2503]	Loss 2.6228e+00 (2.8070e+00)	Acc@1  42.58 ( 39.68)	Acc@5  67.58 ( 64.75)
Epoch: [3][1599/2503]	Loss 2.6579e+00 (2.8041e+00)	Acc@1  45.31 ( 39.74)	Acc@5  70.70 ( 64.80)
Epoch: [3][1699/2503]	Loss 2.8908e+00 (2.8005e+00)	Acc@1  36.33 ( 39.83)	Acc@5  64.84 ( 64.87)
Epoch: [3][1799/2503]	Loss 2.9180e+00 (2.7970e+00)	Acc@1  38.28 ( 39.88)	Acc@5  63.67 ( 64.92)
Epoch: [3][1899/2503]	Loss 2.4370e+00 (2.7928e+00)	Acc@1  44.14 ( 39.95)	Acc@5  70.70 ( 65.00)
Epoch: [3][1999/2503]	Loss 2.7718e+00 (2.7888e+00)	Acc@1  39.45 ( 40.02)	Acc@5  68.36 ( 65.06)
Epoch: [3][2099/2503]	Loss 2.7151e+00 (2.7850e+00)	Acc@1  41.02 ( 40.10)	Acc@5  68.36 ( 65.12)
Epoch: [3][2199/2503]	Loss 2.7039e+00 (2.7806e+00)	Acc@1  41.80 ( 40.17)	Acc@5  68.36 ( 65.18)
Epoch: [3][2299/2503]	Loss 2.5664e+00 (2.7770e+00)	Acc@1  44.92 ( 40.23)	Acc@5  69.92 ( 65.24)
Epoch: [3][2399/2503]	Loss 2.6449e+00 (2.7732e+00)	Acc@1  43.75 ( 40.29)	Acc@5  68.36 ( 65.30)
Epoch: [3][2499/2503]	Loss 2.6279e+00 (2.7691e+00)	Acc@1  42.97 ( 40.36)	Acc@5  67.97 ( 65.37)
 * Train Acc@1 40.358 Train Acc@5 65.369
Epoch: [3][ 99/391]	Loss 2.3149e+00 (2.4368e+00)	Acc@1  48.44 ( 45.22)	Acc@5  79.69 ( 71.95)
Epoch: [3][199/391]	Loss 2.2610e+00 (2.4603e+00)	Acc@1  46.88 ( 44.99)	Acc@5  71.88 ( 71.02)
Epoch: [3][299/391]	Loss 2.2761e+00 (2.4584e+00)	Acc@1  51.56 ( 44.83)	Acc@5  75.00 ( 71.06)
 * Val Acc@1 44.808 Val Acc@5 71.016
Epoch: [4][  99/2503]	Loss 2.6167e+00 (2.5673e+00)	Acc@1  44.53 ( 43.40)	Acc@5  68.36 ( 68.66)
Epoch: [4][ 199/2503]	Loss 2.7161e+00 (2.5848e+00)	Acc@1  39.06 ( 43.41)	Acc@5  65.62 ( 68.55)
Epoch: [4][ 299/2503]	Loss 2.4044e+00 (2.5689e+00)	Acc@1  47.27 ( 43.78)	Acc@5  71.09 ( 68.80)
Epoch: [4][ 399/2503]	Loss 2.6241e+00 (2.5713e+00)	Acc@1  39.45 ( 43.73)	Acc@5  69.92 ( 68.73)
Epoch: [4][ 499/2503]	Loss 2.6918e+00 (2.5735e+00)	Acc@1  42.97 ( 43.69)	Acc@5  67.19 ( 68.70)
Epoch: [4][ 599/2503]	Loss 2.6479e+00 (2.5722e+00)	Acc@1  43.36 ( 43.72)	Acc@5  67.58 ( 68.69)
Epoch: [4][ 699/2503]	Loss 2.4699e+00 (2.5666e+00)	Acc@1  41.41 ( 43.81)	Acc@5  69.14 ( 68.81)
Epoch: [4][ 799/2503]	Loss 2.6949e+00 (2.5651e+00)	Acc@1  43.75 ( 43.86)	Acc@5  67.19 ( 68.82)
Epoch: [4][ 899/2503]	Loss 2.5759e+00 (2.5628e+00)	Acc@1  40.62 ( 43.90)	Acc@5  69.92 ( 68.82)
Epoch: [4][ 999/2503]	Loss 2.3270e+00 (2.5602e+00)	Acc@1  46.09 ( 43.98)	Acc@5  71.09 ( 68.87)
Epoch: [4][1099/2503]	Loss 2.2902e+00 (2.5554e+00)	Acc@1  50.39 ( 44.07)	Acc@5  71.09 ( 68.95)
Epoch: [4][1199/2503]	Loss 2.7094e+00 (2.5513e+00)	Acc@1  42.19 ( 44.14)	Acc@5  66.80 ( 69.01)
Epoch: [4][1299/2503]	Loss 2.7173e+00 (2.5471e+00)	Acc@1  43.36 ( 44.26)	Acc@5  66.02 ( 69.10)
Epoch: [4][1399/2503]	Loss 2.6350e+00 (2.5437e+00)	Acc@1  46.09 ( 44.35)	Acc@5  67.97 ( 69.15)
Epoch: [4][1499/2503]	Loss 2.4671e+00 (2.5388e+00)	Acc@1  44.14 ( 44.45)	Acc@5  69.92 ( 69.24)
Epoch: [4][1599/2503]	Loss 2.5551e+00 (2.5332e+00)	Acc@1  48.83 ( 44.56)	Acc@5  70.70 ( 69.32)
Epoch: [4][1699/2503]	Loss 2.1557e+00 (2.5271e+00)	Acc@1  53.12 ( 44.72)	Acc@5  76.56 ( 69.42)
Epoch: [4][1799/2503]	Loss 2.3818e+00 (2.5209e+00)	Acc@1  49.22 ( 44.83)	Acc@5  71.88 ( 69.50)
Epoch: [4][1899/2503]	Loss 2.1265e+00 (2.5147e+00)	Acc@1  50.78 ( 44.95)	Acc@5  74.22 ( 69.59)
Epoch: [4][1999/2503]	Loss 2.3295e+00 (2.5085e+00)	Acc@1  46.88 ( 45.05)	Acc@5  72.66 ( 69.68)
Epoch: [4][2099/2503]	Loss 2.6757e+00 (2.5023e+00)	Acc@1  42.58 ( 45.18)	Acc@5  67.19 ( 69.78)
Epoch: [4][2199/2503]	Loss 2.2804e+00 (2.4952e+00)	Acc@1  46.09 ( 45.31)	Acc@5  75.00 ( 69.90)
Epoch: [4][2299/2503]	Loss 2.4149e+00 (2.4894e+00)	Acc@1  46.48 ( 45.43)	Acc@5  70.31 ( 69.98)
Epoch: [4][2399/2503]	Loss 2.4047e+00 (2.4838e+00)	Acc@1  48.83 ( 45.54)	Acc@5  69.14 ( 70.05)
Epoch: [4][2499/2503]	Loss 2.2512e+00 (2.4771e+00)	Acc@1  49.22 ( 45.65)	Acc@5  75.00 ( 70.16)
 * Train Acc@1 45.652 Train Acc@5 70.165
Epoch: [4][ 99/391]	Loss 1.8789e+00 (2.0238e+00)	Acc@1  62.50 ( 53.47)	Acc@5  76.56 ( 77.83)
Epoch: [4][199/391]	Loss 1.7128e+00 (2.0443e+00)	Acc@1  64.06 ( 52.35)	Acc@5  81.25 ( 77.46)
Epoch: [4][299/391]	Loss 2.1893e+00 (2.0361e+00)	Acc@1  54.69 ( 52.82)	Acc@5  81.25 ( 77.71)
 * Val Acc@1 52.812 Val Acc@5 77.688
Epoch: [5][  99/2503]	Loss 2.0962e+00 (2.2441e+00)	Acc@1  53.52 ( 49.99)	Acc@5  75.78 ( 74.05)
Epoch: [5][ 199/2503]	Loss 2.1783e+00 (2.2411e+00)	Acc@1  51.95 ( 50.02)	Acc@5  76.56 ( 74.05)
Epoch: [5][ 299/2503]	Loss 2.0850e+00 (2.2266e+00)	Acc@1  51.95 ( 50.38)	Acc@5  75.78 ( 74.28)
Epoch: [5][ 399/2503]	Loss 2.2323e+00 (2.2135e+00)	Acc@1  44.14 ( 50.61)	Acc@5  77.73 ( 74.48)
Epoch: [5][ 499/2503]	Loss 2.1239e+00 (2.2074e+00)	Acc@1  55.86 ( 50.83)	Acc@5  74.61 ( 74.56)
Epoch: [5][ 599/2503]	Loss 2.3679e+00 (2.2022e+00)	Acc@1  50.00 ( 50.87)	Acc@5  69.92 ( 74.65)
Epoch: [5][ 699/2503]	Loss 2.2205e+00 (2.1922e+00)	Acc@1  51.17 ( 51.07)	Acc@5  77.34 ( 74.80)
Epoch: [5][ 799/2503]	Loss 2.2323e+00 (2.1827e+00)	Acc@1  49.22 ( 51.24)	Acc@5  72.27 ( 74.93)
Epoch: [5][ 899/2503]	Loss 2.2361e+00 (2.1751e+00)	Acc@1  51.17 ( 51.38)	Acc@5  76.95 ( 75.05)
Epoch: [5][ 999/2503]	Loss 2.2831e+00 (2.1679e+00)	Acc@1  49.61 ( 51.54)	Acc@5  74.22 ( 75.13)
Epoch: [5][1099/2503]	Loss 2.1992e+00 (2.1617e+00)	Acc@1  50.78 ( 51.67)	Acc@5  75.78 ( 75.23)
Epoch: [5][1199/2503]	Loss 2.2899e+00 (2.1532e+00)	Acc@1  54.30 ( 51.88)	Acc@5  73.83 ( 75.33)
Epoch: [5][1299/2503]	Loss 2.1998e+00 (2.1446e+00)	Acc@1  51.17 ( 52.07)	Acc@5  73.83 ( 75.44)
Epoch: [5][1399/2503]	Loss 2.1500e+00 (2.1371e+00)	Acc@1  51.17 ( 52.24)	Acc@5  74.61 ( 75.55)
Epoch: [5][1499/2503]	Loss 1.8905e+00 (2.1304e+00)	Acc@1  57.03 ( 52.38)	Acc@5  77.73 ( 75.63)
Epoch: [5][1599/2503]	Loss 2.0473e+00 (2.1235e+00)	Acc@1  54.69 ( 52.52)	Acc@5  76.56 ( 75.73)
Epoch: [5][1699/2503]	Loss 2.0097e+00 (2.1166e+00)	Acc@1  58.20 ( 52.63)	Acc@5  78.12 ( 75.84)
Epoch: [5][1799/2503]	Loss 2.2116e+00 (2.1094e+00)	Acc@1  53.12 ( 52.75)	Acc@5  71.48 ( 75.95)
Epoch: [5][1899/2503]	Loss 2.1064e+00 (2.1036e+00)	Acc@1  49.61 ( 52.86)	Acc@5  74.22 ( 76.03)
Epoch: [5][1999/2503]	Loss 2.0718e+00 (2.0976e+00)	Acc@1  53.91 ( 52.99)	Acc@5  75.39 ( 76.12)
Epoch: [5][2099/2503]	Loss 1.9815e+00 (2.0910e+00)	Acc@1  50.78 ( 53.13)	Acc@5  79.69 ( 76.20)
Epoch: [5][2199/2503]	Loss 1.8067e+00 (2.0841e+00)	Acc@1  59.38 ( 53.27)	Acc@5  82.81 ( 76.31)
Epoch: [5][2299/2503]	Loss 1.9973e+00 (2.0778e+00)	Acc@1  54.30 ( 53.39)	Acc@5  78.12 ( 76.41)
Epoch: [5][2399/2503]	Loss 1.8863e+00 (2.0731e+00)	Acc@1  58.59 ( 53.48)	Acc@5  79.30 ( 76.47)
Epoch: [5][2499/2503]	Loss 1.9711e+00 (2.0689e+00)	Acc@1  56.64 ( 53.57)	Acc@5  79.30 ( 76.53)
 * Train Acc@1 53.568 Train Acc@5 76.528
Epoch: [5][ 99/391]	Loss 1.3704e+00 (1.6763e+00)	Acc@1  67.19 ( 60.36)	Acc@5  87.50 ( 82.58)
Epoch: [5][199/391]	Loss 1.3637e+00 (1.6890e+00)	Acc@1  75.00 ( 59.87)	Acc@5  85.94 ( 82.58)
Epoch: [5][299/391]	Loss 1.6216e+00 (1.6860e+00)	Acc@1  60.94 ( 60.26)	Acc@5  84.38 ( 82.78)
 * Val Acc@1 60.012 Val Acc@5 82.660
Training complete in: 3:18:19.927112
(pytorch_p37) ubuntu@ip-172-31-2-157:~/work/v3/week09/hw$ 
