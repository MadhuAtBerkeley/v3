python imagenet-distributed.py  -n 1 -g 1 -nr 0 --epochs 2
=> creating model 'resnet18'
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Epoch: [0][  99/5005]	Loss 6.7641e+00 (6.9626e+00)	Acc@1   0.00 (  0.27)	Acc@5   2.34 (  1.15)
Epoch: [0][ 199/5005]	Loss 6.5874e+00 (6.8159e+00)	Acc@1   1.56 (  0.47)	Acc@5   3.52 (  1.82)
Epoch: [0][ 299/5005]	Loss 6.3260e+00 (6.7059e+00)	Acc@1   1.17 (  0.60)	Acc@5   5.86 (  2.40)
Epoch: [0][ 399/5005]	Loss 6.3154e+00 (6.6077e+00)	Acc@1   1.17 (  0.81)	Acc@5   5.47 (  3.07)
Epoch: [0][ 499/5005]	Loss 6.1501e+00 (6.5250e+00)	Acc@1   0.78 (  0.97)	Acc@5   7.42 (  3.66)
Epoch: [0][ 599/5005]	Loss 6.1252e+00 (6.4509e+00)	Acc@1   1.17 (  1.15)	Acc@5   5.08 (  4.23)
Epoch: [0][ 699/5005]	Loss 6.0509e+00 (6.3855e+00)	Acc@1   2.34 (  1.30)	Acc@5   7.81 (  4.83)
Epoch: [0][ 799/5005]	Loss 5.9343e+00 (6.3240e+00)	Acc@1   1.17 (  1.50)	Acc@5   7.03 (  5.41)
Epoch: [0][ 899/5005]	Loss 5.7385e+00 (6.2655e+00)	Acc@1   4.30 (  1.70)	Acc@5  10.94 (  6.01)
Epoch: [0][ 999/5005]	Loss 5.6869e+00 (6.2092e+00)	Acc@1   3.12 (  1.90)	Acc@5  11.33 (  6.60)
Epoch: [0][1099/5005]	Loss 5.6580e+00 (6.1578e+00)	Acc@1   3.12 (  2.13)	Acc@5  10.55 (  7.22)
Epoch: [0][1199/5005]	Loss 5.5389e+00 (6.1069e+00)	Acc@1   5.08 (  2.35)	Acc@5  12.11 (  7.81)
Epoch: [0][1299/5005]	Loss 5.2638e+00 (6.0580e+00)	Acc@1   7.81 (  2.57)	Acc@5  17.97 (  8.42)
Epoch: [0][1399/5005]	Loss 5.4816e+00 (6.0111e+00)	Acc@1   3.52 (  2.80)	Acc@5  13.67 (  9.02)
Epoch: [0][1499/5005]	Loss 5.4016e+00 (5.9672e+00)	Acc@1   4.30 (  3.01)	Acc@5  16.41 (  9.62)
Epoch: [0][1599/5005]	Loss 5.2245e+00 (5.9250e+00)	Acc@1   8.20 (  3.23)	Acc@5  22.66 ( 10.21)
Epoch: [0][1699/5005]	Loss 5.1079e+00 (5.8820e+00)	Acc@1   8.98 (  3.49)	Acc@5  22.66 ( 10.83)
Epoch: [0][1799/5005]	Loss 4.9952e+00 (5.8417e+00)	Acc@1   9.38 (  3.74)	Acc@5  24.22 ( 11.41)
Epoch: [0][1899/5005]	Loss 5.0365e+00 (5.7997e+00)	Acc@1   7.42 (  3.99)	Acc@5  24.61 ( 12.02)
Epoch: [0][1999/5005]	Loss 4.8460e+00 (5.7603e+00)	Acc@1  11.33 (  4.25)	Acc@5  26.17 ( 12.61)
Epoch: [0][2099/5005]	Loss 4.9852e+00 (5.7217e+00)	Acc@1   8.59 (  4.52)	Acc@5  28.12 ( 13.20)
Epoch: [0][2199/5005]	Loss 5.0049e+00 (5.6834e+00)	Acc@1  12.11 (  4.80)	Acc@5  24.61 ( 13.80)
Epoch: [0][2299/5005]	Loss 4.9211e+00 (5.6461e+00)	Acc@1  12.50 (  5.08)	Acc@5  25.78 ( 14.38)
Epoch: [0][2399/5005]	Loss 4.6660e+00 (5.6088e+00)	Acc@1  12.11 (  5.36)	Acc@5  30.08 ( 14.96)
Epoch: [0][2499/5005]	Loss 4.5992e+00 (5.5725e+00)	Acc@1  17.19 (  5.63)	Acc@5  30.08 ( 15.56)
Epoch: [0][2599/5005]	Loss 4.5838e+00 (5.5370e+00)	Acc@1  13.28 (  5.91)	Acc@5  30.86 ( 16.13)
Epoch: [0][2699/5005]	Loss 4.6922e+00 (5.5025e+00)	Acc@1  14.06 (  6.19)	Acc@5  32.81 ( 16.71)
Epoch: [0][2799/5005]	Loss 4.4832e+00 (5.4678e+00)	Acc@1  15.23 (  6.48)	Acc@5  34.77 ( 17.27)
Epoch: [0][2899/5005]	Loss 4.5061e+00 (5.4348e+00)	Acc@1  17.19 (  6.75)	Acc@5  32.42 ( 17.83)
Epoch: [0][2999/5005]	Loss 4.3980e+00 (5.4025e+00)	Acc@1  16.41 (  7.02)	Acc@5  37.50 ( 18.37)
Epoch: [0][3099/5005]	Loss 4.2328e+00 (5.3703e+00)	Acc@1  16.41 (  7.31)	Acc@5  37.50 ( 18.91)
Epoch: [0][3199/5005]	Loss 4.4602e+00 (5.3387e+00)	Acc@1  15.62 (  7.60)	Acc@5  36.72 ( 19.46)
Epoch: [0][3299/5005]	Loss 4.5457e+00 (5.3081e+00)	Acc@1  14.45 (  7.87)	Acc@5  32.42 ( 19.98)
Epoch: [0][3399/5005]	Loss 4.0627e+00 (5.2775e+00)	Acc@1  22.66 (  8.15)	Acc@5  44.14 ( 20.50)
Epoch: [0][3499/5005]	Loss 4.3365e+00 (5.2476e+00)	Acc@1  17.58 (  8.44)	Acc@5  39.45 ( 21.02)
Epoch: [0][3599/5005]	Loss 4.1307e+00 (5.2188e+00)	Acc@1  18.75 (  8.71)	Acc@5  39.45 ( 21.51)
Epoch: [0][3699/5005]	Loss 4.1774e+00 (5.1909e+00)	Acc@1  16.02 (  8.97)	Acc@5  41.80 ( 22.00)
Epoch: [0][3799/5005]	Loss 3.9940e+00 (5.1628e+00)	Acc@1  19.53 (  9.24)	Acc@5  42.19 ( 22.49)
Epoch: [0][3899/5005]	Loss 4.3099e+00 (5.1362e+00)	Acc@1  17.97 (  9.52)	Acc@5  36.72 ( 22.96)
Epoch: [0][3999/5005]	Loss 3.8438e+00 (5.1104e+00)	Acc@1  21.09 (  9.78)	Acc@5  44.53 ( 23.41)
Epoch: [0][4099/5005]	Loss 4.1456e+00 (5.0844e+00)	Acc@1  19.53 ( 10.04)	Acc@5  40.62 ( 23.87)
Epoch: [0][4199/5005]	Loss 4.4187e+00 (5.0590e+00)	Acc@1  17.19 ( 10.29)	Acc@5  32.42 ( 24.32)
Epoch: [0][4299/5005]	Loss 3.8644e+00 (5.0342e+00)	Acc@1  23.44 ( 10.55)	Acc@5  44.14 ( 24.76)
Epoch: [0][4399/5005]	Loss 4.0868e+00 (5.0093e+00)	Acc@1  16.41 ( 10.80)	Acc@5  42.97 ( 25.20)
Epoch: [0][4499/5005]	Loss 3.8252e+00 (4.9851e+00)	Acc@1  24.22 ( 11.05)	Acc@5  47.66 ( 25.63)
Epoch: [0][4599/5005]	Loss 3.8778e+00 (4.9614e+00)	Acc@1  21.88 ( 11.30)	Acc@5  47.27 ( 26.05)
Epoch: [0][4699/5005]	Loss 3.8413e+00 (4.9382e+00)	Acc@1  21.48 ( 11.54)	Acc@5  43.75 ( 26.46)
Epoch: [0][4799/5005]	Loss 4.0482e+00 (4.9156e+00)	Acc@1  20.70 ( 11.78)	Acc@5  43.75 ( 26.87)
Epoch: [0][4899/5005]	Loss 3.7619e+00 (4.8932e+00)	Acc@1  25.39 ( 12.01)	Acc@5  48.44 ( 27.26)
Epoch: [0][4999/5005]	Loss 3.5632e+00 (4.8710e+00)	Acc@1  25.00 ( 12.26)	Acc@5  53.52 ( 27.67)
 * Train Acc@1 12.269 Train Acc@5 27.688
Epoch: [0][ 99/782]	Loss 3.7206e+00 (3.9463e+00)	Acc@1  28.12 ( 22.33)	Acc@5  46.88 ( 44.17)
Epoch: [0][199/782]	Loss 3.6921e+00 (3.9388e+00)	Acc@1  23.44 ( 22.32)	Acc@5  48.44 ( 44.86)
Epoch: [0][299/782]	Loss 3.8125e+00 (3.9170e+00)	Acc@1  28.12 ( 22.67)	Acc@5  51.56 ( 45.56)
Epoch: [0][399/782]	Loss 3.8597e+00 (3.9232e+00)	Acc@1  23.44 ( 22.62)	Acc@5  46.88 ( 45.35)
Epoch: [0][499/782]	Loss 3.9632e+00 (3.9106e+00)	Acc@1  21.88 ( 22.70)	Acc@5  37.50 ( 45.56)
Epoch: [0][599/782]	Loss 3.9629e+00 (3.9059e+00)	Acc@1  12.50 ( 22.72)	Acc@5  46.88 ( 45.65)
Epoch: [0][699/782]	Loss 3.6703e+00 (3.9014e+00)	Acc@1  20.31 ( 22.76)	Acc@5  50.00 ( 45.69)
 * Val Acc@1 22.692 Val Acc@5 45.598
Epoch: [1][  99/5005]	Loss 3.5479e+00 (3.7340e+00)	Acc@1  21.48 ( 24.64)	Acc@5  50.78 ( 47.76)
Epoch: [1][ 199/5005]	Loss 3.7236e+00 (3.7205e+00)	Acc@1  25.78 ( 24.89)	Acc@5  48.83 ( 48.19)
Epoch: [1][ 299/5005]	Loss 3.5445e+00 (3.7166e+00)	Acc@1  28.91 ( 24.93)	Acc@5  52.73 ( 48.30)
Epoch: [1][ 399/5005]	Loss 3.7908e+00 (3.7100e+00)	Acc@1  25.78 ( 25.02)	Acc@5  44.53 ( 48.44)
Epoch: [1][ 499/5005]	Loss 3.6811e+00 (3.6965e+00)	Acc@1  23.44 ( 25.19)	Acc@5  47.27 ( 48.70)
Epoch: [1][ 599/5005]	Loss 3.4566e+00 (3.6883e+00)	Acc@1  26.95 ( 25.38)	Acc@5  51.56 ( 48.82)
Epoch: [1][ 699/5005]	Loss 3.6722e+00 (3.6829e+00)	Acc@1  25.78 ( 25.48)	Acc@5  49.61 ( 49.00)
Epoch: [1][ 799/5005]	Loss 3.2935e+00 (3.6713e+00)	Acc@1  30.47 ( 25.65)	Acc@5  54.30 ( 49.22)
Epoch: [1][ 899/5005]	Loss 3.6451e+00 (3.6650e+00)	Acc@1  26.56 ( 25.76)	Acc@5  52.73 ( 49.35)
Epoch: [1][ 999/5005]	Loss 3.5756e+00 (3.6553e+00)	Acc@1  26.17 ( 25.93)	Acc@5  49.61 ( 49.52)
Epoch: [1][1099/5005]	Loss 3.4301e+00 (3.6441e+00)	Acc@1  27.34 ( 26.10)	Acc@5  50.00 ( 49.70)
Epoch: [1][1199/5005]	Loss 3.4944e+00 (3.6330e+00)	Acc@1  29.69 ( 26.25)	Acc@5  53.52 ( 49.91)
Epoch: [1][1299/5005]	Loss 3.5065e+00 (3.6221e+00)	Acc@1  25.39 ( 26.41)	Acc@5  53.91 ( 50.10)
Epoch: [1][1399/5005]	Loss 3.3233e+00 (3.6110e+00)	Acc@1  29.30 ( 26.59)	Acc@5  54.69 ( 50.28)
Epoch: [1][1499/5005]	Loss 3.3418e+00 (3.5994e+00)	Acc@1  27.73 ( 26.75)	Acc@5  54.30 ( 50.49)
Epoch: [1][1599/5005]	Loss 3.5079e+00 (3.5894e+00)	Acc@1  26.17 ( 26.91)	Acc@5  48.44 ( 50.68)
Epoch: [1][1699/5005]	Loss 3.3922e+00 (3.5800e+00)	Acc@1  31.25 ( 27.07)	Acc@5  54.69 ( 50.89)
Epoch: [1][1799/5005]	Loss 3.4640e+00 (3.5701e+00)	Acc@1  28.91 ( 27.23)	Acc@5  54.30 ( 51.08)
Epoch: [1][1899/5005]	Loss 3.3442e+00 (3.5602e+00)	Acc@1  27.73 ( 27.38)	Acc@5  55.47 ( 51.27)
Epoch: [1][1999/5005]	Loss 3.6255e+00 (3.5502e+00)	Acc@1  26.17 ( 27.55)	Acc@5  50.39 ( 51.46)
Epoch: [1][2099/5005]	Loss 3.0243e+00 (3.5398e+00)	Acc@1  37.89 ( 27.71)	Acc@5  60.94 ( 51.65)
Epoch: [1][2199/5005]	Loss 3.4063e+00 (3.5289e+00)	Acc@1  27.34 ( 27.89)	Acc@5  55.47 ( 51.85)
Epoch: [1][2299/5005]	Loss 3.2274e+00 (3.5189e+00)	Acc@1  32.03 ( 28.05)	Acc@5  58.20 ( 52.04)
Epoch: [1][2399/5005]	Loss 3.1478e+00 (3.5090e+00)	Acc@1  33.59 ( 28.21)	Acc@5  57.42 ( 52.22)
Epoch: [1][2499/5005]	Loss 3.3195e+00 (3.4978e+00)	Acc@1  25.39 ( 28.38)	Acc@5  58.59 ( 52.44)
Epoch: [1][2599/5005]	Loss 3.0820e+00 (3.4859e+00)	Acc@1  34.77 ( 28.56)	Acc@5  58.98 ( 52.65)
Epoch: [1][2699/5005]	Loss 3.3079e+00 (3.4750e+00)	Acc@1  33.20 ( 28.74)	Acc@5  55.47 ( 52.86)
Epoch: [1][2799/5005]	Loss 3.1988e+00 (3.4637e+00)	Acc@1  33.20 ( 28.92)	Acc@5  59.38 ( 53.06)
Epoch: [1][2899/5005]	Loss 3.0930e+00 (3.4517e+00)	Acc@1  34.38 ( 29.12)	Acc@5  60.16 ( 53.27)
Epoch: [1][2999/5005]	Loss 3.2992e+00 (3.4393e+00)	Acc@1  30.86 ( 29.31)	Acc@5  56.25 ( 53.51)
Epoch: [1][3099/5005]	Loss 3.3291e+00 (3.4267e+00)	Acc@1  30.47 ( 29.51)	Acc@5  54.69 ( 53.73)
Epoch: [1][3199/5005]	Loss 2.8301e+00 (3.4145e+00)	Acc@1  35.94 ( 29.72)	Acc@5  63.28 ( 53.94)
Epoch: [1][3299/5005]	Loss 2.8125e+00 (3.4025e+00)	Acc@1  40.23 ( 29.92)	Acc@5  62.11 ( 54.15)
Epoch: [1][3399/5005]	Loss 3.1473e+00 (3.3902e+00)	Acc@1  32.81 ( 30.13)	Acc@5  58.20 ( 54.36)
Epoch: [1][3499/5005]	Loss 2.8023e+00 (3.3777e+00)	Acc@1  41.02 ( 30.34)	Acc@5  67.58 ( 54.59)
Epoch: [1][3599/5005]	Loss 3.0086e+00 (3.3650e+00)	Acc@1  33.98 ( 30.54)	Acc@5  62.11 ( 54.81)
Epoch: [1][3699/5005]	Loss 2.8689e+00 (3.3523e+00)	Acc@1  43.36 ( 30.76)	Acc@5  64.06 ( 55.05)
Epoch: [1][3799/5005]	Loss 2.8251e+00 (3.3392e+00)	Acc@1  43.75 ( 30.97)	Acc@5  64.06 ( 55.28)
Epoch: [1][3899/5005]	Loss 2.9713e+00 (3.3262e+00)	Acc@1  35.94 ( 31.20)	Acc@5  63.67 ( 55.51)
Epoch: [1][3999/5005]	Loss 2.5531e+00 (3.3126e+00)	Acc@1  42.58 ( 31.42)	Acc@5  68.75 ( 55.74)
Epoch: [1][4099/5005]	Loss 2.6942e+00 (3.2997e+00)	Acc@1  41.41 ( 31.64)	Acc@5  66.41 ( 55.97)
Epoch: [1][4199/5005]	Loss 2.7043e+00 (3.2863e+00)	Acc@1  44.14 ( 31.87)	Acc@5  66.80 ( 56.20)
Epoch: [1][4299/5005]	Loss 2.7623e+00 (3.2731e+00)	Acc@1  42.97 ( 32.10)	Acc@5  67.19 ( 56.44)
Epoch: [1][4399/5005]	Loss 2.2378e+00 (3.2599e+00)	Acc@1  50.78 ( 32.34)	Acc@5  74.22 ( 56.66)
Epoch: [1][4499/5005]	Loss 2.8261e+00 (3.2470e+00)	Acc@1  37.11 ( 32.56)	Acc@5  64.84 ( 56.88)
Epoch: [1][4599/5005]	Loss 2.6394e+00 (3.2343e+00)	Acc@1  46.09 ( 32.78)	Acc@5  69.53 ( 57.11)
Epoch: [1][4699/5005]	Loss 2.8365e+00 (3.2215e+00)	Acc@1  40.62 ( 33.01)	Acc@5  66.41 ( 57.33)
Epoch: [1][4799/5005]	Loss 2.7083e+00 (3.2089e+00)	Acc@1  42.19 ( 33.23)	Acc@5  68.75 ( 57.54)
Epoch: [1][4899/5005]	Loss 2.6992e+00 (3.1971e+00)	Acc@1  39.45 ( 33.44)	Acc@5  67.19 ( 57.75)
Epoch: [1][4999/5005]	Loss 2.6922e+00 (3.1857e+00)	Acc@1  40.62 ( 33.63)	Acc@5  66.41 ( 57.94)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0
 * Train Acc@1 33.637 Train Acc@5 57.948
Epoch: [1][ 99/782]	Loss 2.0330e+00 (2.2473e+00)	Acc@1  51.56 ( 48.86)	Acc@5  76.56 ( 74.20)
Epoch: [1][199/782]	Loss 2.0925e+00 (2.2521e+00)	Acc@1  57.81 ( 49.00)	Acc@5  78.12 ( 74.12)
Epoch: [1][299/782]	Loss 2.4059e+00 (2.2450e+00)	Acc@1  48.44 ( 49.26)	Acc@5  71.88 ( 74.01)
Epoch: [1][399/782]	Loss 2.1897e+00 (2.2614e+00)	Acc@1  45.31 ( 48.91)	Acc@5  75.00 ( 73.78)
Epoch: [1][499/782]	Loss 2.1851e+00 (2.2602e+00)	Acc@1  46.88 ( 48.86)	Acc@5  81.25 ( 73.72)
Epoch: [1][599/782]	Loss 2.3000e+00 (2.2584e+00)	Acc@1  45.31 ( 48.86)	Acc@5  75.00 ( 73.81)
Epoch: [1][699/782]	Loss 2.2460e+00 (2.2532e+00)	Acc@1  51.56 ( 49.06)	Acc@5  75.00 ( 73.86)
 * Val Acc@1 49.090 Val Acc@5 73.862
Training complete in: 1:30:40.328945
