(pytorch_p37) ubuntu@ip-172-31-6-102:~/work/v3/week09/hw$ python imagenet-distributed.py  -n 2 -g 1 -nr 1 --epochs 6
=> creating model 'resnet18'
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [0][  99/2503]	Loss 6.6324e+00 (6.8461e+00)	Acc@1   0.78 (  0.49)	Acc@5   3.12 (  1.84)
Epoch: [0][ 199/2503]	Loss 6.3069e+00 (6.6406e+00)	Acc@1   1.56 (  0.84)	Acc@5   5.47 (  3.08)
Epoch: [0][ 299/2503]	Loss 6.0292e+00 (6.4701e+00)	Acc@1   1.95 (  1.20)	Acc@5   8.20 (  4.32)
Epoch: [0][ 399/2503]	Loss 5.7994e+00 (6.3380e+00)	Acc@1   2.73 (  1.60)	Acc@5  11.72 (  5.54)
Epoch: [0][ 499/2503]	Loss 5.8191e+00 (6.2236e+00)	Acc@1   3.12 (  1.96)	Acc@5  12.89 (  6.73)
Epoch: [0][ 599/2503]	Loss 5.5569e+00 (6.1249e+00)	Acc@1   3.91 (  2.34)	Acc@5  14.84 (  7.82)
Epoch: [0][ 699/2503]	Loss 5.4184e+00 (6.0338e+00)	Acc@1   4.69 (  2.77)	Acc@5  16.02 (  8.89)
Epoch: [0][ 799/2503]	Loss 5.1862e+00 (5.9554e+00)	Acc@1   8.98 (  3.13)	Acc@5  19.53 (  9.86)
Epoch: [0][ 899/2503]	Loss 5.3933e+00 (5.8815e+00)	Acc@1   6.25 (  3.54)	Acc@5  15.62 ( 10.85)
Epoch: [0][ 999/2503]	Loss 5.1785e+00 (5.8100e+00)	Acc@1   7.81 (  3.96)	Acc@5  20.31 ( 11.87)
Epoch: [0][1099/2503]	Loss 4.8611e+00 (5.7418e+00)	Acc@1  11.33 (  4.38)	Acc@5  27.73 ( 12.84)
Epoch: [0][1199/2503]	Loss 4.8892e+00 (5.6767e+00)	Acc@1   9.38 (  4.81)	Acc@5  26.17 ( 13.83)
Epoch: [0][1299/2503]	Loss 4.9377e+00 (5.6134e+00)	Acc@1   7.81 (  5.28)	Acc@5  24.22 ( 14.84)
Epoch: [0][1399/2503]	Loss 4.8118e+00 (5.5532e+00)	Acc@1   8.98 (  5.72)	Acc@5  26.95 ( 15.79)
Epoch: [0][1499/2503]	Loss 4.7299e+00 (5.4938e+00)	Acc@1  10.16 (  6.17)	Acc@5  30.08 ( 16.75)
Epoch: [0][1599/2503]	Loss 4.8476e+00 (5.4354e+00)	Acc@1  12.50 (  6.63)	Acc@5  28.91 ( 17.71)
Epoch: [0][1699/2503]	Loss 4.4433e+00 (5.3813e+00)	Acc@1  14.06 (  7.09)	Acc@5  31.64 ( 18.61)
Epoch: [0][1799/2503]	Loss 4.2108e+00 (5.3279e+00)	Acc@1  19.14 (  7.54)	Acc@5  39.06 ( 19.49)
Epoch: [0][1899/2503]	Loss 4.1504e+00 (5.2760e+00)	Acc@1  20.70 (  8.01)	Acc@5  42.58 ( 20.37)
Epoch: [0][1999/2503]	Loss 4.0762e+00 (5.2271e+00)	Acc@1  16.80 (  8.46)	Acc@5  42.19 ( 21.22)
Epoch: [0][2099/2503]	Loss 4.1798e+00 (5.1786e+00)	Acc@1  18.36 (  8.92)	Acc@5  41.41 ( 22.07)
Epoch: [0][2199/2503]	Loss 4.3482e+00 (5.1316e+00)	Acc@1  16.41 (  9.35)	Acc@5  37.50 ( 22.88)
Epoch: [0][2299/2503]	Loss 4.0965e+00 (5.0863e+00)	Acc@1  16.80 (  9.80)	Acc@5  41.80 ( 23.67)
Epoch: [0][2399/2503]	Loss 4.1407e+00 (5.0419e+00)	Acc@1  18.75 ( 10.24)	Acc@5  38.67 ( 24.45)
Epoch: [0][2499/2503]	Loss 3.7059e+00 (4.9990e+00)	Acc@1  26.56 ( 10.68)	Acc@5  49.61 ( 25.21)
 * Train Acc@1 10.694 Train Acc@5 25.230
Epoch: [0][ 99/391]	Loss 4.4774e+00 (4.1270e+00)	Acc@1  20.31 ( 18.64)	Acc@5  40.62 ( 40.30)
Epoch: [0][199/391]	Loss 3.6530e+00 (4.1216e+00)	Acc@1  21.88 ( 19.00)	Acc@5  48.44 ( 40.66)
Epoch: [0][299/391]	Loss 4.2177e+00 (4.1154e+00)	Acc@1  23.44 ( 19.04)	Acc@5  42.19 ( 40.84)
 * Val Acc@1 18.936 Val Acc@5 41.052
Epoch: [1][  99/2503]	Loss 3.8015e+00 (3.9164e+00)	Acc@1  26.56 ( 22.11)	Acc@5  48.05 ( 44.58)
Epoch: [1][ 199/2503]	Loss 3.8930e+00 (3.8856e+00)	Acc@1  23.83 ( 22.53)	Acc@5  44.14 ( 45.19)
Epoch: [1][ 299/2503]	Loss 3.8638e+00 (3.8571e+00)	Acc@1  20.70 ( 22.77)	Acc@5  43.36 ( 45.62)
Epoch: [1][ 399/2503]	Loss 3.6325e+00 (3.8371e+00)	Acc@1  26.56 ( 23.08)	Acc@5  49.22 ( 45.98)
Epoch: [1][ 499/2503]	Loss 3.6643e+00 (3.8214e+00)	Acc@1  24.22 ( 23.44)	Acc@5  48.83 ( 46.32)
Epoch: [1][ 599/2503]	Loss 3.4954e+00 (3.8016e+00)	Acc@1  26.56 ( 23.69)	Acc@5  51.56 ( 46.72)
Epoch: [1][ 699/2503]	Loss 3.6728e+00 (3.7820e+00)	Acc@1  23.05 ( 23.93)	Acc@5  49.61 ( 47.11)
Epoch: [1][ 799/2503]	Loss 3.7858e+00 (3.7633e+00)	Acc@1  24.22 ( 24.26)	Acc@5  50.00 ( 47.46)
Epoch: [1][ 899/2503]	Loss 3.5638e+00 (3.7465e+00)	Acc@1  20.70 ( 24.50)	Acc@5  53.52 ( 47.78)
Epoch: [1][ 999/2503]	Loss 3.6559e+00 (3.7316e+00)	Acc@1  26.56 ( 24.68)	Acc@5  46.88 ( 48.06)
Epoch: [1][1099/2503]	Loss 3.5264e+00 (3.7166e+00)	Acc@1  25.78 ( 24.92)	Acc@5  51.56 ( 48.36)
Epoch: [1][1199/2503]	Loss 3.3887e+00 (3.7019e+00)	Acc@1  29.30 ( 25.14)	Acc@5  49.61 ( 48.63)
Epoch: [1][1299/2503]	Loss 3.5905e+00 (3.6840e+00)	Acc@1  25.39 ( 25.41)	Acc@5  51.56 ( 48.93)
Epoch: [1][1399/2503]	Loss 3.3116e+00 (3.6675e+00)	Acc@1  26.56 ( 25.64)	Acc@5  55.08 ( 49.24)
Epoch: [1][1499/2503]	Loss 3.5426e+00 (3.6509e+00)	Acc@1  27.73 ( 25.90)	Acc@5  48.83 ( 49.54)
Epoch: [1][1599/2503]	Loss 3.4074e+00 (3.6348e+00)	Acc@1  29.30 ( 26.15)	Acc@5  55.08 ( 49.85)
Epoch: [1][1699/2503]	Loss 3.1778e+00 (3.6203e+00)	Acc@1  30.86 ( 26.37)	Acc@5  57.03 ( 50.13)
Epoch: [1][1799/2503]	Loss 3.2484e+00 (3.6066e+00)	Acc@1  33.59 ( 26.60)	Acc@5  55.47 ( 50.37)
Epoch: [1][1899/2503]	Loss 3.2987e+00 (3.5925e+00)	Acc@1  31.64 ( 26.81)	Acc@5  53.52 ( 50.62)
Epoch: [1][1999/2503]	Loss 3.4822e+00 (3.5794e+00)	Acc@1  28.91 ( 27.02)	Acc@5  47.66 ( 50.86)
Epoch: [1][2099/2503]	Loss 3.2912e+00 (3.5662e+00)	Acc@1  30.47 ( 27.22)	Acc@5  57.03 ( 51.10)
Epoch: [1][2199/2503]	Loss 3.0726e+00 (3.5535e+00)	Acc@1  29.30 ( 27.42)	Acc@5  59.38 ( 51.33)
Epoch: [1][2299/2503]	Loss 3.3567e+00 (3.5410e+00)	Acc@1  33.59 ( 27.62)	Acc@5  56.25 ( 51.57)
Epoch: [1][2399/2503]	Loss 3.0948e+00 (3.5278e+00)	Acc@1  35.55 ( 27.82)	Acc@5  58.98 ( 51.81)
Epoch: [1][2499/2503]	Loss 3.1970e+00 (3.5158e+00)	Acc@1  35.16 ( 28.01)	Acc@5  58.59 ( 52.02)
 * Train Acc@1 28.012 Train Acc@5 52.030
Epoch: [1][ 99/391]	Loss 3.5383e+00 (3.2865e+00)	Acc@1  31.25 ( 31.36)	Acc@5  53.12 ( 56.61)
Epoch: [1][199/391]	Loss 2.7415e+00 (3.2883e+00)	Acc@1  40.62 ( 31.48)	Acc@5  62.50 ( 56.91)
Epoch: [1][299/391]	Loss 3.2055e+00 (3.2899e+00)	Acc@1  35.94 ( 31.21)	Acc@5  62.50 ( 56.77)
 * Val Acc@1 31.212 Val Acc@5 57.060
Epoch: [2][  99/2503]	Loss 3.3468e+00 (3.1565e+00)	Acc@1  33.20 ( 33.63)	Acc@5  53.12 ( 58.57)
Epoch: [2][ 199/2503]	Loss 3.2106e+00 (3.1528e+00)	Acc@1  35.16 ( 33.67)	Acc@5  55.08 ( 58.59)
Epoch: [2][ 299/2503]	Loss 3.1365e+00 (3.1501e+00)	Acc@1  30.86 ( 33.80)	Acc@5  58.59 ( 58.59)
Epoch: [2][ 399/2503]	Loss 3.3325e+00 (3.1482e+00)	Acc@1  31.25 ( 33.68)	Acc@5  53.91 ( 58.62)
Epoch: [2][ 499/2503]	Loss 3.2686e+00 (3.1411e+00)	Acc@1  33.20 ( 33.79)	Acc@5  56.64 ( 58.72)
Epoch: [2][ 599/2503]	Loss 3.1577e+00 (3.1347e+00)	Acc@1  33.20 ( 33.92)	Acc@5  58.59 ( 58.90)
Epoch: [2][ 699/2503]	Loss 3.0907e+00 (3.1285e+00)	Acc@1  34.77 ( 34.08)	Acc@5  60.16 ( 59.04)
Epoch: [2][ 799/2503]	Loss 3.0207e+00 (3.1212e+00)	Acc@1  33.20 ( 34.20)	Acc@5  58.59 ( 59.20)
Epoch: [2][ 899/2503]	Loss 3.1630e+00 (3.1161e+00)	Acc@1  34.77 ( 34.29)	Acc@5  59.77 ( 59.30)
Epoch: [2][ 999/2503]	Loss 3.0969e+00 (3.1094e+00)	Acc@1  33.20 ( 34.40)	Acc@5  60.16 ( 59.41)
Epoch: [2][1099/2503]	Loss 3.2765e+00 (3.1068e+00)	Acc@1  33.20 ( 34.44)	Acc@5  54.30 ( 59.46)
Epoch: [2][1199/2503]	Loss 3.1710e+00 (3.1011e+00)	Acc@1  35.94 ( 34.55)	Acc@5  57.42 ( 59.58)
Epoch: [2][1299/2503]	Loss 3.1794e+00 (3.0935e+00)	Acc@1  31.25 ( 34.66)	Acc@5  55.86 ( 59.71)
Epoch: [2][1399/2503]	Loss 2.7978e+00 (3.0852e+00)	Acc@1  42.58 ( 34.80)	Acc@5  68.75 ( 59.87)
Epoch: [2][1499/2503]	Loss 3.0402e+00 (3.0789e+00)	Acc@1  33.59 ( 34.89)	Acc@5  59.77 ( 59.98)
Epoch: [2][1599/2503]	Loss 2.9501e+00 (3.0746e+00)	Acc@1  36.33 ( 34.97)	Acc@5  62.50 ( 60.05)
Epoch: [2][1699/2503]	Loss 2.9495e+00 (3.0681e+00)	Acc@1  39.45 ( 35.10)	Acc@5  58.98 ( 60.16)
Epoch: [2][1799/2503]	Loss 2.9860e+00 (3.0645e+00)	Acc@1  37.50 ( 35.17)	Acc@5  59.77 ( 60.21)
Epoch: [2][1899/2503]	Loss 3.0292e+00 (3.0594e+00)	Acc@1  34.77 ( 35.25)	Acc@5  60.55 ( 60.30)
Epoch: [2][1999/2503]	Loss 2.9261e+00 (3.0529e+00)	Acc@1  42.58 ( 35.37)	Acc@5  62.89 ( 60.42)
Epoch: [2][2099/2503]	Loss 3.1733e+00 (3.0486e+00)	Acc@1  32.03 ( 35.46)	Acc@5  55.47 ( 60.49)
Epoch: [2][2199/2503]	Loss 2.8616e+00 (3.0448e+00)	Acc@1  37.50 ( 35.53)	Acc@5  64.84 ( 60.58)
Epoch: [2][2299/2503]	Loss 2.9667e+00 (3.0389e+00)	Acc@1  35.16 ( 35.63)	Acc@5  64.06 ( 60.69)
Epoch: [2][2399/2503]	Loss 2.9981e+00 (3.0332e+00)	Acc@1  35.55 ( 35.71)	Acc@5  62.89 ( 60.79)
Epoch: [2][2499/2503]	Loss 2.8634e+00 (3.0277e+00)	Acc@1  36.72 ( 35.81)	Acc@5  66.41 ( 60.88)
 * Train Acc@1 35.814 Train Acc@5 60.886
Epoch: [2][ 99/391]	Loss 3.0234e+00 (2.8274e+00)	Acc@1  37.50 ( 38.81)	Acc@5  59.38 ( 64.00)
Epoch: [2][199/391]	Loss 2.3601e+00 (2.7898e+00)	Acc@1  48.44 ( 39.33)	Acc@5  75.00 ( 65.09)
Epoch: [2][299/391]	Loss 3.2779e+00 (2.8108e+00)	Acc@1  29.69 ( 38.49)	Acc@5  60.94 ( 64.73)
 * Val Acc@1 38.584 Val Acc@5 64.904
Epoch: [3][  99/2503]	Loss 2.9662e+00 (2.8473e+00)	Acc@1  33.20 ( 38.64)	Acc@5  59.38 ( 64.48)
Epoch: [3][ 199/2503]	Loss 3.1336e+00 (2.8478e+00)	Acc@1  32.03 ( 38.64)	Acc@5  58.20 ( 64.31)
Epoch: [3][ 299/2503]	Loss 2.8712e+00 (2.8446e+00)	Acc@1  39.84 ( 38.85)	Acc@5  62.50 ( 64.29)
Epoch: [3][ 399/2503]	Loss 2.8443e+00 (2.8432e+00)	Acc@1  39.84 ( 38.91)	Acc@5  62.89 ( 64.28)
Epoch: [3][ 499/2503]	Loss 2.7578e+00 (2.8365e+00)	Acc@1  38.28 ( 39.01)	Acc@5  62.50 ( 64.34)
Epoch: [3][ 599/2503]	Loss 3.1094e+00 (2.8333e+00)	Acc@1  36.33 ( 39.07)	Acc@5  61.72 ( 64.40)
Epoch: [3][ 699/2503]	Loss 2.8481e+00 (2.8283e+00)	Acc@1  38.28 ( 39.21)	Acc@5  62.50 ( 64.44)
Epoch: [3][ 799/2503]	Loss 2.8231e+00 (2.8242e+00)	Acc@1  39.06 ( 39.29)	Acc@5  60.94 ( 64.52)
Epoch: [3][ 899/2503]	Loss 2.8769e+00 (2.8235e+00)	Acc@1  40.62 ( 39.30)	Acc@5  67.58 ( 64.51)
Epoch: [3][ 999/2503]	Loss 2.8125e+00 (2.8188e+00)	Acc@1  40.62 ( 39.42)	Acc@5  64.45 ( 64.56)
Epoch: [3][1099/2503]	Loss 2.6500e+00 (2.8194e+00)	Acc@1  44.92 ( 39.43)	Acc@5  68.36 ( 64.54)
Epoch: [3][1199/2503]	Loss 2.7508e+00 (2.8148e+00)	Acc@1  45.31 ( 39.50)	Acc@5  63.67 ( 64.65)
Epoch: [3][1299/2503]	Loss 2.6545e+00 (2.8125e+00)	Acc@1  43.36 ( 39.56)	Acc@5  66.80 ( 64.68)
Epoch: [3][1399/2503]	Loss 2.6428e+00 (2.8090e+00)	Acc@1  40.62 ( 39.63)	Acc@5  69.14 ( 64.75)
Epoch: [3][1499/2503]	Loss 2.6430e+00 (2.8047e+00)	Acc@1  42.19 ( 39.71)	Acc@5  67.58 ( 64.82)
Epoch: [3][1599/2503]	Loss 3.0409e+00 (2.8014e+00)	Acc@1  37.11 ( 39.77)	Acc@5  59.77 ( 64.88)
Epoch: [3][1699/2503]	Loss 2.5151e+00 (2.7969e+00)	Acc@1  43.36 ( 39.87)	Acc@5  70.70 ( 64.94)
Epoch: [3][1799/2503]	Loss 2.9358e+00 (2.7928e+00)	Acc@1  37.89 ( 39.96)	Acc@5  59.77 ( 65.00)
Epoch: [3][1899/2503]	Loss 2.6418e+00 (2.7895e+00)	Acc@1  45.70 ( 40.03)	Acc@5  67.19 ( 65.06)
Epoch: [3][1999/2503]	Loss 2.7945e+00 (2.7852e+00)	Acc@1  40.62 ( 40.10)	Acc@5  62.89 ( 65.14)
Epoch: [3][2099/2503]	Loss 2.6385e+00 (2.7823e+00)	Acc@1  41.02 ( 40.15)	Acc@5  68.36 ( 65.19)
Epoch: [3][2199/2503]	Loss 2.7285e+00 (2.7779e+00)	Acc@1  42.97 ( 40.23)	Acc@5  66.80 ( 65.25)
Epoch: [3][2299/2503]	Loss 2.8316e+00 (2.7742e+00)	Acc@1  39.84 ( 40.31)	Acc@5  65.23 ( 65.32)
Epoch: [3][2399/2503]	Loss 2.7439e+00 (2.7697e+00)	Acc@1  38.67 ( 40.39)	Acc@5  65.62 ( 65.39)
Epoch: [3][2499/2503]	Loss 2.6888e+00 (2.7668e+00)	Acc@1  41.41 ( 40.45)	Acc@5  65.62 ( 65.44)
 * Train Acc@1 40.451 Train Acc@5 65.445
Epoch: [3][ 99/391]	Loss 2.5119e+00 (2.4747e+00)	Acc@1  43.75 ( 44.12)	Acc@5  71.88 ( 70.47)
Epoch: [3][199/391]	Loss 2.1650e+00 (2.4601e+00)	Acc@1  46.88 ( 44.44)	Acc@5  78.12 ( 70.72)
Epoch: [3][299/391]	Loss 2.5611e+00 (2.4608e+00)	Acc@1  46.88 ( 44.23)	Acc@5  70.31 ( 70.59)
 * Val Acc@1 44.500 Val Acc@5 70.840
Epoch: [4][  99/2503]	Loss 2.6281e+00 (2.5916e+00)	Acc@1  43.75 ( 43.40)	Acc@5  66.02 ( 68.30)
Epoch: [4][ 199/2503]	Loss 2.4828e+00 (2.5849e+00)	Acc@1  44.53 ( 43.53)	Acc@5  71.09 ( 68.37)
Epoch: [4][ 299/2503]	Loss 2.3570e+00 (2.5919e+00)	Acc@1  48.44 ( 43.42)	Acc@5  74.61 ( 68.31)
Epoch: [4][ 399/2503]	Loss 2.4648e+00 (2.5968e+00)	Acc@1  47.27 ( 43.44)	Acc@5  72.27 ( 68.20)
Epoch: [4][ 499/2503]	Loss 2.8294e+00 (2.5919e+00)	Acc@1  38.28 ( 43.51)	Acc@5  66.02 ( 68.29)
Epoch: [4][ 599/2503]	Loss 2.8101e+00 (2.5919e+00)	Acc@1  40.23 ( 43.48)	Acc@5  62.50 ( 68.29)
Epoch: [4][ 699/2503]	Loss 2.4989e+00 (2.5831e+00)	Acc@1  47.27 ( 43.65)	Acc@5  69.92 ( 68.44)
Epoch: [4][ 799/2503]	Loss 2.6069e+00 (2.5766e+00)	Acc@1  39.84 ( 43.81)	Acc@5  71.48 ( 68.55)
Epoch: [4][ 899/2503]	Loss 2.5678e+00 (2.5711e+00)	Acc@1  44.14 ( 43.92)	Acc@5  70.31 ( 68.64)
Epoch: [4][ 999/2503]	Loss 2.3342e+00 (2.5669e+00)	Acc@1  46.09 ( 44.00)	Acc@5  73.05 ( 68.71)
Epoch: [4][1099/2503]	Loss 2.6049e+00 (2.5648e+00)	Acc@1  44.14 ( 44.02)	Acc@5  67.19 ( 68.75)
Epoch: [4][1199/2503]	Loss 2.5068e+00 (2.5585e+00)	Acc@1  42.58 ( 44.14)	Acc@5  70.70 ( 68.84)
Epoch: [4][1299/2503]	Loss 2.5620e+00 (2.5524e+00)	Acc@1  47.27 ( 44.23)	Acc@5  70.31 ( 68.92)
Epoch: [4][1399/2503]	Loss 2.4765e+00 (2.5476e+00)	Acc@1  49.22 ( 44.32)	Acc@5  67.19 ( 69.02)
Epoch: [4][1499/2503]	Loss 2.4345e+00 (2.5423e+00)	Acc@1  44.53 ( 44.43)	Acc@5  70.70 ( 69.11)
Epoch: [4][1599/2503]	Loss 2.3749e+00 (2.5373e+00)	Acc@1  44.92 ( 44.52)	Acc@5  74.61 ( 69.20)
Epoch: [4][1699/2503]	Loss 2.5297e+00 (2.5328e+00)	Acc@1  45.31 ( 44.64)	Acc@5  67.58 ( 69.28)
Epoch: [4][1799/2503]	Loss 2.5406e+00 (2.5253e+00)	Acc@1  45.70 ( 44.77)	Acc@5  66.80 ( 69.41)
Epoch: [4][1899/2503]	Loss 2.4269e+00 (2.5181e+00)	Acc@1  49.22 ( 44.91)	Acc@5  71.09 ( 69.51)
Epoch: [4][1999/2503]	Loss 2.0154e+00 (2.5125e+00)	Acc@1  50.00 ( 45.01)	Acc@5  76.17 ( 69.60)
Epoch: [4][2099/2503]	Loss 2.2340e+00 (2.5057e+00)	Acc@1  48.83 ( 45.15)	Acc@5  72.27 ( 69.70)
Epoch: [4][2199/2503]	Loss 2.2359e+00 (2.5000e+00)	Acc@1  50.78 ( 45.26)	Acc@5  73.05 ( 69.80)
Epoch: [4][2299/2503]	Loss 2.2568e+00 (2.4934e+00)	Acc@1  49.61 ( 45.36)	Acc@5  74.61 ( 69.90)
Epoch: [4][2399/2503]	Loss 2.0340e+00 (2.4866e+00)	Acc@1  57.42 ( 45.49)	Acc@5  76.95 ( 70.00)
Epoch: [4][2499/2503]	Loss 2.0149e+00 (2.4795e+00)	Acc@1  56.64 ( 45.63)	Acc@5  76.95 ( 70.11)
 * Train Acc@1 45.629 Train Acc@5 70.109
Epoch: [4][ 99/391]	Loss 2.1718e+00 (2.0416e+00)	Acc@1  53.12 ( 52.72)	Acc@5  76.56 ( 77.17)
Epoch: [4][199/391]	Loss 1.9283e+00 (2.0428e+00)	Acc@1  53.12 ( 52.87)	Acc@5  79.69 ( 77.38)
Epoch: [4][299/391]	Loss 2.0491e+00 (2.0475e+00)	Acc@1  50.00 ( 52.68)	Acc@5  78.12 ( 77.40)
 * Val Acc@1 53.144 Val Acc@5 77.540
Epoch: [5][  99/2503]	Loss 2.3224e+00 (2.2242e+00)	Acc@1  46.88 ( 50.07)	Acc@5  75.00 ( 74.18)
Epoch: [5][ 199/2503]	Loss 2.3111e+00 (2.2283e+00)	Acc@1  46.48 ( 50.17)	Acc@5  70.70 ( 74.08)
Epoch: [5][ 299/2503]	Loss 2.1319e+00 (2.2161e+00)	Acc@1  51.95 ( 50.58)	Acc@5  76.95 ( 74.29)
Epoch: [5][ 399/2503]	Loss 2.2268e+00 (2.2055e+00)	Acc@1  49.22 ( 50.79)	Acc@5  73.83 ( 74.42)
Epoch: [5][ 499/2503]	Loss 2.3797e+00 (2.1957e+00)	Acc@1  51.95 ( 50.97)	Acc@5  74.61 ( 74.54)
Epoch: [5][ 599/2503]	Loss 2.2412e+00 (2.1895e+00)	Acc@1  50.39 ( 51.07)	Acc@5  73.44 ( 74.60)
Epoch: [5][ 699/2503]	Loss 2.3017e+00 (2.1828e+00)	Acc@1  48.83 ( 51.20)	Acc@5  71.88 ( 74.70)
Epoch: [5][ 799/2503]	Loss 2.2230e+00 (2.1762e+00)	Acc@1  46.48 ( 51.37)	Acc@5  75.00 ( 74.78)
Epoch: [5][ 899/2503]	Loss 2.2564e+00 (2.1686e+00)	Acc@1  49.61 ( 51.51)	Acc@5  74.22 ( 74.90)
Epoch: [5][ 999/2503]	Loss 2.0652e+00 (2.1597e+00)	Acc@1  58.20 ( 51.66)	Acc@5  77.73 ( 75.05)
Epoch: [5][1099/2503]	Loss 2.1214e+00 (2.1517e+00)	Acc@1  50.39 ( 51.82)	Acc@5  76.56 ( 75.17)
Epoch: [5][1199/2503]	Loss 1.8875e+00 (2.1447e+00)	Acc@1  58.59 ( 51.99)	Acc@5  79.69 ( 75.26)
Epoch: [5][1299/2503]	Loss 2.2703e+00 (2.1363e+00)	Acc@1  53.12 ( 52.20)	Acc@5  76.56 ( 75.36)
Epoch: [5][1399/2503]	Loss 1.9560e+00 (2.1289e+00)	Acc@1  54.30 ( 52.35)	Acc@5  76.95 ( 75.48)
Epoch: [5][1499/2503]	Loss 1.7823e+00 (2.1209e+00)	Acc@1  60.55 ( 52.53)	Acc@5  80.08 ( 75.61)
Epoch: [5][1599/2503]	Loss 2.0408e+00 (2.1129e+00)	Acc@1  55.08 ( 52.67)	Acc@5  78.91 ( 75.74)
Epoch: [5][1699/2503]	Loss 1.8265e+00 (2.1065e+00)	Acc@1  59.77 ( 52.81)	Acc@5  80.86 ( 75.85)
Epoch: [5][1799/2503]	Loss 2.1364e+00 (2.1012e+00)	Acc@1  53.52 ( 52.91)	Acc@5  76.17 ( 75.93)
Epoch: [5][1899/2503]	Loss 1.7050e+00 (2.0943e+00)	Acc@1  58.20 ( 53.05)	Acc@5  82.03 ( 76.04)
Epoch: [5][1999/2503]	Loss 1.8412e+00 (2.0891e+00)	Acc@1  56.25 ( 53.17)	Acc@5  78.12 ( 76.12)
Epoch: [5][2099/2503]	Loss 1.7200e+00 (2.0829e+00)	Acc@1  57.03 ( 53.27)	Acc@5  83.20 ( 76.21)
Epoch: [5][2199/2503]	Loss 1.9459e+00 (2.0783e+00)	Acc@1  56.64 ( 53.37)	Acc@5  77.34 ( 76.27)
Epoch: [5][2299/2503]	Loss 1.7952e+00 (2.0741e+00)	Acc@1  58.98 ( 53.45)	Acc@5  76.95 ( 76.34)
Epoch: [5][2399/2503]	Loss 2.1373e+00 (2.0693e+00)	Acc@1  51.17 ( 53.56)	Acc@5  76.17 ( 76.41)
Epoch: [5][2499/2503]	Loss 1.8218e+00 (2.0651e+00)	Acc@1  58.98 ( 53.64)	Acc@5  79.69 ( 76.47)
 * Train Acc@1 53.638 Train Acc@5 76.472
Epoch: [5][ 99/391]	Loss 1.9239e+00 (1.6863e+00)	Acc@1  65.62 ( 61.12)	Acc@5  84.38 ( 82.48)
Epoch: [5][199/391]	Loss 1.3517e+00 (1.6947e+00)	Acc@1  65.62 ( 60.44)	Acc@5  85.94 ( 82.60)
Epoch: [5][299/391]	Loss 1.8635e+00 (1.7000e+00)	Acc@1  56.25 ( 60.10)	Acc@5  79.69 ( 82.57)
 * Val Acc@1 60.532 Val Acc@5 82.620
Training complete in: 3:18:20.609760
(pytorch_p37) ubuntu@ip-172-31-6-102:~/work/v3/week09/hw$ 