(pytorch_latest_p37) ubuntu@ip-172-31-2-157:~/work/v3/week09/hw$ python imagenet-distributed.py  -n 2 -g 1 -nr 0 --epochs 2
=> creating model 'resnet18'
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Epoch: [0][  99/2503]	Loss 6.7485e+00 (6.8519e+00)	Acc@1   0.78 (  0.38)	Acc@5   1.56 (  1.62)
Epoch: [0][ 199/2503]	Loss 6.2154e+00 (6.6391e+00)	Acc@1   3.12 (  0.79)	Acc@5   7.81 (  3.06)
Epoch: [0][ 299/2503]	Loss 5.9773e+00 (6.4831e+00)	Acc@1   1.95 (  1.15)	Acc@5   7.81 (  4.19)
Epoch: [0][ 399/2503]	Loss 5.8717e+00 (6.3589e+00)	Acc@1   4.30 (  1.51)	Acc@5  10.94 (  5.33)
Epoch: [0][ 499/2503]	Loss 5.7478e+00 (6.2445e+00)	Acc@1   2.73 (  1.87)	Acc@5  10.55 (  6.42)
Epoch: [0][ 599/2503]	Loss 5.4903e+00 (6.1448e+00)	Acc@1   3.91 (  2.24)	Acc@5  14.84 (  7.53)
Epoch: [0][ 699/2503]	Loss 5.3039e+00 (6.0502e+00)	Acc@1   5.08 (  2.67)	Acc@5  17.58 (  8.72)
Epoch: [0][ 799/2503]	Loss 5.2812e+00 (5.9632e+00)	Acc@1   5.08 (  3.10)	Acc@5  20.31 (  9.82)
Epoch: [0][ 899/2503]	Loss 4.9231e+00 (5.8809e+00)	Acc@1   8.20 (  3.56)	Acc@5  25.78 ( 10.99)
Epoch: [0][ 999/2503]	Loss 4.9045e+00 (5.8017e+00)	Acc@1   9.77 (  4.06)	Acc@5  25.00 ( 12.11)
Epoch: [0][1099/2503]	Loss 5.0651e+00 (5.7266e+00)	Acc@1   8.59 (  4.54)	Acc@5  22.66 ( 13.24)
Epoch: [0][1199/2503]	Loss 4.6265e+00 (5.6520e+00)	Acc@1  12.11 (  5.05)	Acc@5  30.47 ( 14.37)
Epoch: [0][1299/2503]	Loss 4.6371e+00 (5.5796e+00)	Acc@1  13.28 (  5.60)	Acc@5  30.86 ( 15.53)
Epoch: [0][1399/2503]	Loss 4.5190e+00 (5.5114e+00)	Acc@1  14.06 (  6.14)	Acc@5  32.42 ( 16.63)
Epoch: [0][1499/2503]	Loss 4.4660e+00 (5.4467e+00)	Acc@1  14.45 (  6.68)	Acc@5  33.98 ( 17.70)
Epoch: [0][1599/2503]	Loss 4.3068e+00 (5.3820e+00)	Acc@1  17.58 (  7.23)	Acc@5  35.16 ( 18.79)
Epoch: [0][1699/2503]	Loss 4.4520e+00 (5.3217e+00)	Acc@1  15.62 (  7.77)	Acc@5  33.20 ( 19.82)
Epoch: [0][1799/2503]	Loss 4.3123e+00 (5.2636e+00)	Acc@1  17.19 (  8.30)	Acc@5  37.11 ( 20.82)
Epoch: [0][1899/2503]	Loss 3.9595e+00 (5.2078e+00)	Acc@1  19.53 (  8.83)	Acc@5  39.45 ( 21.79)
Epoch: [0][1999/2503]	Loss 4.0863e+00 (5.1546e+00)	Acc@1  16.41 (  9.34)	Acc@5  40.23 ( 22.71)
Epoch: [0][2099/2503]	Loss 4.1215e+00 (5.1027e+00)	Acc@1  19.14 (  9.84)	Acc@5  39.84 ( 23.62)
Epoch: [0][2199/2503]	Loss 4.1573e+00 (5.0527e+00)	Acc@1  17.97 ( 10.35)	Acc@5  37.89 ( 24.51)
Epoch: [0][2299/2503]	Loss 3.8373e+00 (5.0033e+00)	Acc@1  21.48 ( 10.85)	Acc@5  44.53 ( 25.38)
Epoch: [0][2399/2503]	Loss 3.9635e+00 (4.9569e+00)	Acc@1  21.88 ( 11.35)	Acc@5  43.36 ( 26.22)
Epoch: [0][2499/2503]	Loss 3.8349e+00 (4.9104e+00)	Acc@1  26.17 ( 11.84)	Acc@5  46.88 ( 27.05)
 * Train Acc@1 11.849 Train Acc@5 27.069
Epoch: [0][ 99/391]	Loss 4.2773e+00 (4.0176e+00)	Acc@1  23.44 ( 20.16)	Acc@5  35.94 ( 43.02)
Epoch: [0][199/391]	Loss 3.8673e+00 (4.0096e+00)	Acc@1  20.31 ( 20.68)	Acc@5  45.31 ( 43.56)
Epoch: [0][299/391]	Loss 4.0015e+00 (4.0044e+00)	Acc@1  25.00 ( 21.03)	Acc@5  40.62 ( 43.59)
 * Val Acc@1 21.052 Val Acc@5 43.304
Epoch: [1][  99/2503]	Loss 3.4781e+00 (3.7272e+00)	Acc@1  26.95 ( 24.84)	Acc@5  52.73 ( 48.22)
Epoch: [1][ 199/2503]	Loss 3.5544e+00 (3.7165e+00)	Acc@1  26.56 ( 25.09)	Acc@5  51.56 ( 48.36)
Epoch: [1][ 299/2503]	Loss 3.6040e+00 (3.6947e+00)	Acc@1  26.56 ( 25.38)	Acc@5  49.61 ( 48.70)
Epoch: [1][ 399/2503]	Loss 3.5902e+00 (3.6723e+00)	Acc@1  28.52 ( 25.60)	Acc@5  49.22 ( 49.12)
Epoch: [1][ 499/2503]	Loss 3.1441e+00 (3.6478e+00)	Acc@1  33.98 ( 25.96)	Acc@5  60.16 ( 49.64)
Epoch: [1][ 599/2503]	Loss 3.4699e+00 (3.6229e+00)	Acc@1  29.69 ( 26.34)	Acc@5  52.73 ( 50.15)
Epoch: [1][ 699/2503]	Loss 3.4506e+00 (3.6027e+00)	Acc@1  26.95 ( 26.67)	Acc@5  52.34 ( 50.48)
Epoch: [1][ 799/2503]	Loss 3.2597e+00 (3.5837e+00)	Acc@1  33.59 ( 26.95)	Acc@5  55.08 ( 50.82)
Epoch: [1][ 899/2503]	Loss 3.2725e+00 (3.5593e+00)	Acc@1  29.30 ( 27.36)	Acc@5  57.42 ( 51.26)
Epoch: [1][ 999/2503]	Loss 3.2335e+00 (3.5376e+00)	Acc@1  32.03 ( 27.72)	Acc@5  55.86 ( 51.65)
Epoch: [1][1099/2503]	Loss 3.2797e+00 (3.5161e+00)	Acc@1  33.98 ( 28.07)	Acc@5  58.20 ( 52.05)
Epoch: [1][1199/2503]	Loss 3.1981e+00 (3.4958e+00)	Acc@1  30.86 ( 28.39)	Acc@5  57.81 ( 52.42)
Epoch: [1][1299/2503]	Loss 3.0826e+00 (3.4734e+00)	Acc@1  30.47 ( 28.70)	Acc@5  59.77 ( 52.84)
Epoch: [1][1399/2503]	Loss 2.9415e+00 (3.4518e+00)	Acc@1  37.50 ( 29.04)	Acc@5  61.72 ( 53.24)
Epoch: [1][1499/2503]	Loss 3.0559e+00 (3.4276e+00)	Acc@1  36.33 ( 29.44)	Acc@5  58.20 ( 53.67)
Epoch: [1][1599/2503]	Loss 3.0921e+00 (3.4047e+00)	Acc@1  30.47 ( 29.79)	Acc@5  63.67 ( 54.07)
Epoch: [1][1699/2503]	Loss 2.9954e+00 (3.3812e+00)	Acc@1  36.72 ( 30.18)	Acc@5  58.98 ( 54.49)
Epoch: [1][1799/2503]	Loss 2.8975e+00 (3.3581e+00)	Acc@1  37.11 ( 30.57)	Acc@5  65.23 ( 54.90)
Epoch: [1][1899/2503]	Loss 2.9403e+00 (3.3349e+00)	Acc@1  36.72 ( 30.97)	Acc@5  61.72 ( 55.30)
Epoch: [1][1999/2503]	Loss 2.7550e+00 (3.3114e+00)	Acc@1  41.41 ( 31.38)	Acc@5  62.89 ( 55.71)
Epoch: [1][2099/2503]	Loss 2.7680e+00 (3.2878e+00)	Acc@1  39.84 ( 31.79)	Acc@5  62.50 ( 56.14)
Epoch: [1][2199/2503]	Loss 2.7398e+00 (3.2648e+00)	Acc@1  41.80 ( 32.21)	Acc@5  67.19 ( 56.55)
Epoch: [1][2299/2503]	Loss 2.7125e+00 (3.2432e+00)	Acc@1  41.02 ( 32.58)	Acc@5  66.02 ( 56.92)
Epoch: [1][2399/2503]	Loss 2.5962e+00 (3.2215e+00)	Acc@1  45.70 ( 32.96)	Acc@5  67.97 ( 57.29)
Epoch: [1][2499/2503]	Loss 2.9736e+00 (3.2018e+00)	Acc@1  37.89 ( 33.29)	Acc@5  58.98 ( 57.63)
 * Train Acc@1 33.299 Train Acc@5 57.636
Epoch: [1][ 99/391]	Loss 2.3042e+00 (2.3465e+00)	Acc@1  43.75 ( 46.72)	Acc@5  67.19 ( 72.42)
Epoch: [1][199/391]	Loss 2.1752e+00 (2.3714e+00)	Acc@1  50.00 ( 46.56)	Acc@5  79.69 ( 71.86)
Epoch: [1][299/391]	Loss 2.2729e+00 (2.3622e+00)	Acc@1  46.88 ( 46.95)	Acc@5  75.00 ( 72.05)
 * Val Acc@1 46.856 Val Acc@5 72.008
Training complete in: 0:48:43.895248